{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n de texto con base en el contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "from functools import partial\n",
    "from typing import Generator\n",
    "\n",
    "from bs4 import BeautifulSoup, Doctype, NavigableString, SoupStrainer, Tag\n",
    "from html2text import HTML2Text\n",
    "from IPython.core.display import Markdown\n",
    "from langchain.document_loaders import DocugamiLoader, RecursiveUrlLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset y funci√≥n de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_url = \"https://python.langchain.com/docs/get_started/quickstart\"\n",
    "\n",
    "load_documents = partial(\n",
    "    RecursiveUrlLoader,\n",
    "    url=doc_url,\n",
    "    max_depth=3,\n",
    "    prevent_outside=True,\n",
    "    check_response_status=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto sin tener en cuenta el contexto\n",
    "\n",
    "La primera aproximaci√≥n para extraer texto de una p√°gina web es simplemente obtener el texto de todos los elementos de la p√°gina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Quickstart | ü¶úÔ∏èüîó Langchain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videosü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmi\n"
     ]
    }
   ],
   "source": [
    "def webpage_text_extractor(html: str) -> str:\n",
    "    return BeautifulSoup(html, \"lxml\").get_text()\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=webpage_text_extractor,\n",
    ")\n",
    "\n",
    "docs_without_data_context = loader.load()\n",
    "print(docs_without_data_context[0].page_content[:520])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto teniendo un poco de contexto\n",
    "\n",
    "El texto de la documentaci√≥n de `Langchain` est√° escrito en `Markdown` y por lo tanto, tiene una estructura que puede ser aprovechada para extraer el texto de manera m√°s precisa.\n",
    "\n",
    "Utilicemos una librer√≠a que nos permita convertir el texto de `HTML` a `Markdown` y as√≠ poder extraer el texto de manera m√°s precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip to main content\n",
      "\n",
      "[ **ü¶úÔ∏èüîó LangChain**](/)[Docs](/docs/get_started/introduction)[Use\n",
      "cases](/docs/use_cases)[Integrations](/docs/integrations/providers)[Guides](/docs/guides/debugging)[API](https://api.python.langchain.com)\n",
      "\n",
      "More\n",
      "\n",
      "  * [Versioning](/docs/packages)\n",
      "  * [Changelog](/docs/changelog)\n",
      "  * [Developer's guide](/docs/contributing)\n",
      "  * [Templates](/docs/templates/)\n",
      "  * [Cookbooks](https://github.com/langchain-ai/langchain/blob/master/cookbook/README.md)\n",
      "  * [Tutorials](/docs/additional_resources/tutorials)\n",
      "  * [YouTube videos](/docs/additional_resources/youtube)\n",
      "\n",
      "ü¶úÔ∏èüîó\n",
      "\n",
      "  * [LangSmith](https://smith.langchain.com)\n",
      "  * [LangSmith Docs](https://docs.smith.langchain.com/)\n",
      "  * [LangServe GitHub](https://git\n"
     ]
    }
   ],
   "source": [
    "def markdown_extractor(html:str)->str:\n",
    "    html2text=HTML2Text()\n",
    "    html2text.ignore_links=False\n",
    "    html2text.ignore_images=False\n",
    "    return html2text.handle(html)\n",
    "\n",
    "loader=load_documents(extractor=markdown_extractor)\n",
    "\n",
    "docs_with_a_bit_of_context = loader.load()\n",
    "print(docs_with_a_bit_of_context[0].page_content[:720])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto teniendo en cuenta el contexto\n",
    "\n",
    "Si bien, cuando utilizamos una librera√≠a para convertir el texto de `HTML` a `Markdown` pudimos extraer el texto de manera m√°s precisa, a√∫n hay algunos casos en los que no se logra extraer el texto de manera correcta.\n",
    "\n",
    "Es aqu√≠ donde entra en juego el dominio del problema. Con base en el conocimiento que tenemos del problema, podemos crear una funci√≥n que nos permita extraer el texto de manera m√°s precisa.\n",
    "\n",
    "Imagina que `langchain_docs_extractor` es como un obrero especializado en una f√°brica cuyo trabajo es transformar materias primas (documentos HTML) en un producto terminado (un string limpio y formateado). Este obrero usa una herramienta especial, `get_text`, como una m√°quina para procesar las materias primas en piezas utilizables, examinando cada componente de la materia prima **pieza por pieza**, y usa el mismo proceso repetidamente (**recursividad**) para descomponer los componentes en su forma m√°s simple. Al final, ensambla todas las piezas procesadas en un producto completo y hace algunos refinamientos finales antes de que el producto salga de la f√°brica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_docs_extractor(html: str,include_output_cells: bool,path_url: str | None = None) -> str:\n",
    "    soup = BeautifulSoup(html,\"lxml\",parse_only=SoupStrainer(name=\"article\"))\n",
    "\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "\n",
    "    # get_text() method returns the text of the tag and all its children.\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child.get_text()\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    text = child.get_text(strip=False)\n",
    "\n",
    "                    if text == \"API Reference:\":\n",
    "                        yield f\"> **{text}**\\n\"\n",
    "                        ul = child.find_next_sibling(\"ul\")\n",
    "                        if ul is not None and isinstance(ul, Tag):\n",
    "                            ul.attrs[\"api_reference\"] = \"true\"\n",
    "                    else:\n",
    "                        yield f\"{'#' * int(child.name[1:])} \"\n",
    "                        yield from child.get_text(strip=False)\n",
    "\n",
    "                        if path_url is not None:\n",
    "                            link = child.find(\"a\")\n",
    "                            if link is not None:\n",
    "                                yield f\" [](/{path_url}/{link.get('href')})\"\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        if language in [\"pycon\", \"text\"] and not include_output_cells:\n",
    "                            continue\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    if \"api_reference\" in child.attrs:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"> - \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    else:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"- \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"table\":\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup))\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=load_documents(extractor=partial(langchain_docs_extractor,include_output_cells=True))\n",
    "docs_with_data_context=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo de salida es ahora en formato Markdown, lo que permite visualizarlo en cualquier editor de texto o en GitHub, ofreciendo una estructura de la informaci√≥n m√°s clara y accesible. Esta organizaci√≥n permite realizar cortes de texto con mayor precisi√≥n, facilitando as√≠ la obtenci√≥n de informaci√≥n m√°s pertinente y relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Quickstart\n",
       "\n",
       "In this quickstart we'll show you how to:\n",
       "\n",
       "- Get setup with LangChain, LangSmith and LangServe\n",
       "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers\n",
       "- Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining\n",
       "- Build a simple application with LangChain\n",
       "- Trace your application with LangSmith\n",
       "- Serve your application with LangServe\n",
       "\n",
       "That's a fair amount to cover! Let's dive in.\n",
       "\n",
       "## Setup‚Äã\n",
       "\n",
       "### Jupyter Notebook‚Äã\n",
       "\n",
       "This guide (and most of the other guides in the documentation) use [Jupyter notebooks](https://jupyter.org/) and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n",
       "\n",
       "You do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See [here](https://jupyter.org/install) for instructions on how to install.\n",
       "\n",
       "### Installation‚Äã\n",
       "\n",
       "To install LangChain run:\n",
       "\n",
       "Pip\n",
       "```bash\n",
       "pip install langchain\n",
       "```\n",
       "\n",
       "Conda\n",
       "```bash\n",
       "conda install langchain -c conda-forge\n",
       "```\n",
       "\n",
       "For more details, see our [Installation guide](/docs/get_started/installation).\n",
       "\n",
       "### LangSmith‚Äã\n",
       "\n",
       "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
       "As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
       "The best way to do this is with [LangSmith](https://smith.langchain.com).\n",
       "\n",
       "Note that LangSmith is not needed, but it is helpful.\n",
       "If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
       "\n",
       "```shell\n",
       "export LANGCHAIN_TRACING_V2=\"true\"\n",
       "export LANGCHAIN_API_KEY=\"...\"\n",
       "```\n",
       "\n",
       "## Building with LangChain‚Äã\n",
       "\n",
       "LangChain enables building application that connect external sources of data and computation to LLMs.\n",
       "In this quickstart, we will walk through a few different ways of doing that.\n",
       "We will start with a simple LLM chain, which just relies on information in the prompt template to respond.\n",
       "Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template.\n",
       "We will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions.\n",
       "Finally, we will build an agent - which utilizes an LLM to determine whether or not it needs to fetch data to answer questions.\n",
       "We will cover these at a high level, but there are lot of details to all of these!\n",
       "We will link to relevant docs.\n",
       "\n",
       "## LLM Chain‚Äã\n",
       "\n",
       "For this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.\n",
       "\n",
       "OpenAI\n",
       "First we'll need to import the LangChain x OpenAI integration package.\n",
       "\n",
       "```shell\n",
       "pip install langchain-openai\n",
       "```\n",
       "\n",
       "Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:\n",
       "\n",
       "```shell\n",
       "export OPENAI_API_KEY=\"...\"\n",
       "```\n",
       "\n",
       "We can then initialize the model:\n",
       "\n",
       "```python\n",
       "from langchain_openai import ChatOpenAI\n",
       "\n",
       "llm = ChatOpenAI()\n",
       "```\n",
       "\n",
       "If you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\n",
       "\n",
       "```python\n",
       "from langchain_openai import ChatOpenAI\n",
       "\n",
       "llm = ChatOpenAI(openai_api_key=\"...\")\n",
       "```\n",
       "\n",
       "Local\n",
       "[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 2, locally.\n",
       "\n",
       "First, follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance:\n",
       "\n",
       "- [Download](https://ollama.ai/download)\n",
       "- Fetch a model via `ollama pull llama2`\n",
       "\n",
       "Then, make sure the Ollama server is running. After that, you can do:\n",
       "\n",
       "```python\n",
       "from langchain_community.llms import Ollama\n",
       "llm = Ollama(model=\"llama2\")\n",
       "```\n",
       "\n",
       "Once you've installed and initialized the LLM of your choice, we can try using it!\n",
       "Let's ask it what LangSmith is - this is something that wasn't present in the training data so it shouldn't have a very good response.\n",
       "\n",
       "```python\n",
       "llm.invoke(\"how can langsmith help with testing?\")\n",
       "```\n",
       "\n",
       "We can also guide it's response with a prompt template.\n",
       "Prompt templates are used to convert raw user input to a better input to the LLM.\n",
       "\n",
       "```python\n",
       "from langchain_core.prompts import ChatPromptTemplate\n",
       "prompt = ChatPromptTemplate.from_messages([\n",
       "    (\"system\", \"You are world class technical documentation writer.\"),\n",
       "    (\"user\", \"{input}\")\n",
       "])\n",
       "```\n",
       "\n",
       "We can now combine these into a simple LLM chain:\n",
       "\n",
       "```python\n",
       "chain = prompt | llm \n",
       "```\n",
       "\n",
       "We can now invoke it and ask the same question. It still won't know the answer, but it should respond in a more proper tone for a technical writer!\n",
       "\n",
       "```python\n",
       "chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
       "```\n",
       "\n",
       "The output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.\n",
       "\n",
       "```python\n",
       "from langchain_core.output_parsers import StrOutputParser\n",
       "\n",
       "output_parser = StrOutputParser()\n",
       "```\n",
       "\n",
       "We can now add this to the previous chain:\n",
       "\n",
       "```python\n",
       "chain = prompt | llm | output_parser\n",
       "```\n",
       "\n",
       "We can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).\n",
       "\n",
       "```python\n",
       "chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
       "```\n",
       "\n",
       "### Diving Deeper‚Äã\n",
       "\n",
       "We've now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see [this section of documentation](/docs/modules/model_io).\n",
       "\n",
       "## Retrieval Chain‚Äã\n",
       "\n",
       "In order to properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM.\n",
       "We can do this via _retrieval_.\n",
       "Retrieval is useful when you have **too much data** to pass to the LLM directly.\n",
       "You can then use a retriever to fetch only the most relevant pieces and pass those in.\n",
       "\n",
       "In this process, we will look up relevant documents from a _Retriever_ and then pass them into the prompt.\n",
       "A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see [this documentation](/docs/modules/data_connection/vectorstores).\n",
       "\n",
       "First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. This requires installing [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/):\n",
       "\n",
       "```text\n",
       "```shell\n",
       "pip install beautifulsoup4\n",
       "```\n",
       "\n",
       "After that, we can import and use WebBaseLoader.\n",
       "\n",
       "```python\n",
       "from langchain_community.document_loaders import WebBaseLoader\n",
       "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
       "\n",
       "docs = loader.load()\n",
       "```\n",
       "\n",
       "Next, we need to index it into a vectorstore. This requires a few components, namely an [embedding model](/docs/modules/data_connection/text_embedding) and a [vectorstore](/docs/modules/data_connection/vectorstores).\n",
       "\n",
       "For embedding models, we once again provide examples for accessing via OpenAI or via local models.\n",
       "\n",
       "OpenAI\n",
       "Make sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).```python\n",
       "from langchain_openai import OpenAIEmbeddings\n",
       "\n",
       "embeddings = OpenAIEmbeddings()\n",
       "```\n",
       "\n",
       "Local\n",
       "Make sure you have Ollama running (same set up as with the LLM).\n",
       "\n",
       "```python\n",
       "from langchain_community.embeddings import OllamaEmbeddings\n",
       "\n",
       "embeddings = OllamaEmbeddings()\n",
       "```\n",
       "\n",
       "Now, we can use this embedding model to ingest documents into a vectorstore.\n",
       "We will use a simple local vectorstore, [FAISS](/docs/integrations/vectorstores/faiss), for simplicity's sake.\n",
       "\n",
       "First we need to install the required packages for that:\n",
       "\n",
       "```shell\n",
       "pip install faiss-cpu\n",
       "```\n",
       "\n",
       "Then we can build our index:\n",
       "\n",
       "```python\n",
       "from langchain_community.vectorstores import FAISS\n",
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "\n",
       "text_splitter = RecursiveCharacterTextSplitter()\n",
       "documents = text_splitter.split_documents(docs)\n",
       "vector = FAISS.from_documents(documents, embeddings)\n",
       "```\n",
       "\n",
       "Now that we have this data indexed in a vectorstore, we will create a retrieval chain.\n",
       "This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.\n",
       "\n",
       "First, let's set up the chain that takes a question and the retrieved documents and generates an answer.\n",
       "\n",
       "```python\n",
       "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
       "\n",
       "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
       "\n",
       "<context>\n",
       "{context}\n",
       "</context>\n",
       "\n",
       "Question: {input}\"\"\")\n",
       "\n",
       "document_chain = create_stuff_documents_chain(llm, prompt)\n",
       "```\n",
       "\n",
       "If we wanted to, we could run this ourselves by passing in documents directly:\n",
       "\n",
       "```python\n",
       "from langchain_core.documents import Document\n",
       "\n",
       "document_chain.invoke({\n",
       "    \"input\": \"how can langsmith help with testing?\",\n",
       "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
       "})\n",
       "```\n",
       "\n",
       "However, we want the documents to first come from the retriever we just set up.\n",
       "That way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.\n",
       "\n",
       "```python\n",
       "from langchain.chains import create_retrieval_chain\n",
       "\n",
       "retriever = vector.as_retriever()\n",
       "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
       "```\n",
       "\n",
       "We can now invoke this chain. This returns a dictionary - the response from the LLM is in the `answer` key\n",
       "\n",
       "```python\n",
       "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
       "print(response[\"answer\"])\n",
       "\n",
       "# LangSmith offers several features that can help with testing:...\n",
       "```\n",
       "\n",
       "This answer should be much more accurate!\n",
       "\n",
       "### Diving Deeper‚Äã\n",
       "\n",
       "We've now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see [this section of documentation](/docs/modules/data_connection).\n",
       "\n",
       "## Conversation Retrieval Chain‚Äã\n",
       "\n",
       "The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?\n",
       "\n",
       "We can still use the `create_retrieval_chain` function, but we need to change two things:\n",
       "\n",
       "1. The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.\n",
       "\n",
       "2. The final LLM chain should likewise take the whole history into account\n",
       "\n",
       "**Updating Retrieval**\n",
       "\n",
       "In order to update retrieval, we will create a new chain. This chain will take in the most recent input (`input`) and the conversation history (`chat_history`) and use an LLM to generate a search query.\n",
       "\n",
       "```python\n",
       "from langchain.chains import create_history_aware_retriever\n",
       "from langchain_core.prompts import MessagesPlaceholder\n",
       "\n",
       "# First we need a prompt that we can pass into an LLM to generate this search query\n",
       "\n",
       "prompt = ChatPromptTemplate.from_messages([\n",
       "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
       "    (\"user\", \"{input}\"),\n",
       "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
       "])\n",
       "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
       "```\n",
       "\n",
       "We can test this out by passing in an instance where the user is asking a follow up question.\n",
       "\n",
       "```python\n",
       "from langchain_core.messages import HumanMessage, AIMessage\n",
       "\n",
       "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
       "retriever_chain.invoke({\n",
       "    \"chat_history\": chat_history,\n",
       "    \"input\": \"Tell me how\"\n",
       "})\n",
       "```\n",
       "\n",
       "You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.\n",
       "\n",
       "Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.\n",
       "\n",
       "```python\n",
       "prompt = ChatPromptTemplate.from_messages([\n",
       "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
       "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
       "    (\"user\", \"{input}\"),\n",
       "])\n",
       "document_chain = create_stuff_documents_chain(llm, prompt)\n",
       "\n",
       "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
       "```\n",
       "\n",
       "We can now test this out end-to-end:\n",
       "\n",
       "```python\n",
       "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
       "retrieval_chain.invoke({\n",
       "    \"chat_history\": chat_history,\n",
       "    \"input\": \"Tell me how\"\n",
       "})\n",
       "```\n",
       "\n",
       "We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!\n",
       "\n",
       "## Agent‚Äã\n",
       "\n",
       "We've so far create examples of chains - where each step is known ahead of time.\n",
       "The final thing we will create is an agent - where the LLM decides what steps to take.\n",
       "\n",
       "**NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.**\n",
       "\n",
       "One of the first things to do when building an agent is to decide what tools it should have access to.\n",
       "For this example, we will give the agent access two tools:\n",
       "\n",
       "1. The retriever we just created. This will let it easily answer questions about LangSmith\n",
       "\n",
       "2. A search tool. This will let it easily answer questions that require up to date information.\n",
       "\n",
       "First, let's set up a tool for the retriever we just created:\n",
       "\n",
       "```python\n",
       "from langchain.tools.retriever import create_retriever_tool\n",
       "\n",
       "retriever_tool = create_retriever_tool(\n",
       "    retriever,\n",
       "    \"langsmith_search\",\n",
       "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
       ")\n",
       "```\n",
       "\n",
       "The search tool that we will use is [Tavily](/docs/integrations/retrievers/tavily). This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:\n",
       "\n",
       "```shell\n",
       "export TAVILY_API_KEY=...\n",
       "```\n",
       "\n",
       "If you do not want to set up an API key, you can skip creating this tool.\n",
       "\n",
       "```python\n",
       "from langchain_community.tools.tavily_search import TavilySearchResults\n",
       "\n",
       "search = TavilySearchResults()\n",
       "```\n",
       "\n",
       "We can now create a list of the tools we want to work with:\n",
       "\n",
       "```python\n",
       "tools = [retriever_tool, search]\n",
       "```\n",
       "\n",
       "Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the [Agent's Getting Started documentation](/docs/modules/agents)\n",
       "\n",
       "Install langchain hub first\n",
       "\n",
       "```bash\n",
       "pip install langchainhub\n",
       "```\n",
       "\n",
       "Now we can use it to get a predefined prompt\n",
       "\n",
       "```python\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain import hub\n",
       "from langchain.agents import create_openai_functions_agent\n",
       "from langchain.agents import AgentExecutor\n",
       "\n",
       "# Get the prompt to use - you can modify this!\n",
       "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
       "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
       "agent = create_openai_functions_agent(llm, tools, prompt)\n",
       "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
       "```\n",
       "\n",
       "We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:\n",
       "\n",
       "```python\n",
       "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
       "```\n",
       "\n",
       "We can ask it about the weather:\n",
       "\n",
       "```python\n",
       "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})\n",
       "```\n",
       "\n",
       "We can have conversations with it:\n",
       "\n",
       "```python\n",
       "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
       "agent_executor.invoke({\n",
       "    \"chat_history\": chat_history,\n",
       "    \"input\": \"Tell me how\"\n",
       "})\n",
       "```\n",
       "\n",
       "### Diving Deeper‚Äã\n",
       "\n",
       "We've now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see [this section of documentation](/docs/modules/agents).\n",
       "\n",
       "## Serving with LangServe‚Äã\n",
       "\n",
       "Now that we've built an application, we need to serve it. That's where LangServe comes in.\n",
       "LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.\n",
       "\n",
       "While the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.\n",
       "\n",
       "Install with:\n",
       "\n",
       "```bash\n",
       "pip install \"langserve[all]\"\n",
       "```\n",
       "\n",
       "### Server‚Äã\n",
       "\n",
       "To create a server for our application we'll make a `serve.py` file. This will contain our logic for serving our application. It consists of three things:\n",
       "\n",
       "1. The definition of our chain that we just built above\n",
       "\n",
       "2. Our FastAPI app\n",
       "\n",
       "3. A definition of a route from which to serve the chain, which is done with `langserve.add_routes`\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "from typing import List\n",
       "\n",
       "from fastapi import FastAPI\n",
       "from langchain_core.prompts import ChatPromptTemplate\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain_community.document_loaders import WebBaseLoader\n",
       "from langchain_openai import OpenAIEmbeddings\n",
       "from langchain_community.vectorstores import FAISS\n",
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "from langchain.tools.retriever import create_retriever_tool\n",
       "from langchain_community.tools.tavily_search import TavilySearchResults\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain import hub\n",
       "from langchain.agents import create_openai_functions_agent\n",
       "from langchain.agents import AgentExecutor\n",
       "from langchain.pydantic_v1 import BaseModel, Field\n",
       "from langchain_core.messages import BaseMessage\n",
       "from langserve import add_routes\n",
       "\n",
       "# 1. Load Retriever\n",
       "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
       "docs = loader.load()\n",
       "text_splitter = RecursiveCharacterTextSplitter()\n",
       "documents = text_splitter.split_documents(docs)\n",
       "embeddings = OpenAIEmbeddings()\n",
       "vector = FAISS.from_documents(documents, embeddings)\n",
       "retriever = vector.as_retriever()\n",
       "\n",
       "# 2. Create Tools\n",
       "retriever_tool = create_retriever_tool(\n",
       "    retriever,\n",
       "    \"langsmith_search\",\n",
       "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
       ")\n",
       "search = TavilySearchResults()\n",
       "tools = [retriever_tool, search]\n",
       "\n",
       "# 3. Create Agent\n",
       "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
       "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
       "agent = create_openai_functions_agent(llm, tools, prompt)\n",
       "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
       "\n",
       "# 4. App definition\n",
       "app = FastAPI(\n",
       "  title=\"LangChain Server\",\n",
       "  version=\"1.0\",\n",
       "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
       ")\n",
       "\n",
       "# 5. Adding chain route\n",
       "\n",
       "# We need to add these input/output schemas because the current AgentExecutor\n",
       "# is lacking in schemas.\n",
       "\n",
       "class Input(BaseModel):\n",
       "    input: str\n",
       "    chat_history: List[BaseMessage] = Field(\n",
       "        ...,\n",
       "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},\n",
       "    )\n",
       "\n",
       "class Output(BaseModel):\n",
       "    output: str\n",
       "\n",
       "add_routes(\n",
       "    app,\n",
       "    agent_executor.with_types(input_type=Input, output_type=Output),\n",
       "    path=\"/agent\",\n",
       ")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    import uvicorn\n",
       "\n",
       "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
       "```\n",
       "\n",
       "And that's it! If we execute this file:\n",
       "\n",
       "```bash\n",
       "python serve.py\n",
       "```\n",
       "\n",
       "we should see our chain being served at localhost:8000.\n",
       "\n",
       "### Playground‚Äã\n",
       "\n",
       "Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps.\n",
       "Head to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.\n",
       "\n",
       "### Client‚Äã\n",
       "\n",
       "Now let's set up a client for programmatically interacting with our service. We can easily do this with the `[langserve.RemoteRunnable](/docs/langserve#client)`.\n",
       "Using this, we can interact with the served chain as if it were running client-side.\n",
       "\n",
       "```python\n",
       "from langserve import RemoteRunnable\n",
       "\n",
       "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
       "remote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
       "```\n",
       "\n",
       "To learn more about the many other features of LangServe [head here](/docs/langserve).\n",
       "\n",
       "## Next steps‚Äã\n",
       "\n",
       "We've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe.\n",
       "There are a lot more features in all three of these than we can cover here.\n",
       "To continue on your journey, we recommend you read the following (in order):\n",
       "\n",
       "- All of these features are backed by [LangChain Expression Language (LCEL)](/docs/expression_language) - a way to chain these components together. Check out that documentation to better understand how to create custom chains.\n",
       "- [Model IO](/docs/modules/model_io) covers more details of prompts, LLMs, and output parsers.\n",
       "- [Retrieval](/docs/modules/data_connection) covers more details of everything related to retrieval\n",
       "- [Agents](/docs/modules/agents) covers details of everything related to agents\n",
       "- Explore common [end-to-end use cases](/docs/use_cases/) and [template applications](/docs/templates)\n",
       "- [Read up on LangSmith](/docs/langsmith/), the platform for debugging, testing, monitoring and more\n",
       "- Learn more about serving your applications with [LangServe](/docs/langserve)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs_with_data_context[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF / DOCX / DOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, vamos a emplear algunos archivos de muestra proporcionados por [Docugami](https://www.docugami.com/). Dichos archivos representan el producto de la extracci√≥n de texto de documentos aut√©nticos, en particular, de archivos PDF relativos a contratos de arrendamiento comercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/docugami/commercial_lease/TruTone Lane 5.xml'),\n",
       " PosixPath('data/docugami/commercial_lease/TruTone Lane 3.xml'),\n",
       " PosixPath('data/docugami/commercial_lease/TruTone Lane 2.xml'),\n",
       " PosixPath('data/docugami/commercial_lease/TruTone Lane 6.xml'),\n",
       " PosixPath('data/docugami/commercial_lease/TruTone Lane 1.xml'),\n",
       " PosixPath('data/docugami/commercial_lease/TruTone Lane 4.xml')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lease_data_dir = pathlib.Path(\"./data/docugami/commercial_lease\")\n",
    "lease_files = list(lease_data_dir.glob(\"*\"))\n",
    "lease_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, carguemos los documentos de muestra y veamos qu√© propiedades tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded 1130 documents'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=DocugamiLoader(docset_id=None,access_token=None,document_ids=None,file_paths=lease_files)\n",
    "\n",
    "lease_docs=loader.load()\n",
    "\n",
    "f\"Loaded {len(lease_docs)} documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metadata obtenida del documento incluye los siguientes elementos:\n",
    "\n",
    "- `id`, `source_id` y `name`: Estos campos identifican de manera un√≠voca al documento y al fragmento de texto que se ha extra√≠do de √©l.\n",
    "- `xpath`: Es el `XPath` correspondiente dentro de la representaci√≥n XML del documento. Se refiere espec√≠ficamente al fragmento extra√≠do. Este campo es √∫til para referenciar directamente las citas del fragmento real dentro del documento XML.\n",
    "- `structure`: Incluye los atributos estructurales del fragmento, tales como `p`, `h1`, `div`, `table`, `td`, entre otros. Es √∫til para filtrar ciertos tipos de fragmentos, en caso de que el usuario los requiera.\n",
    "- `tag`: Representa la etiqueta sem√°ntica para el fragmento. Se genera utilizando diversas t√©cnicas, tanto generativas como extractivas, para determinar el significado del fragmento en cuesti√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xpath': '/docset:RIDERTOLEASE-section/dg:chunk',\n",
       " 'id': 'f807c0113ccbb1a2a82e9eedbbe0c527',\n",
       " 'name': 'TruTone Lane 5.xml',\n",
       " 'source': 'TruTone Lane 5.xml',\n",
       " 'structure': 'h1 p',\n",
       " 'tag': 'chunk IncorporationofTerms'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lease_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docugami` tambi√©n posee la capacidad de asistir en la extracci√≥n de metadatos espec√≠ficos para cada `chunk` o fragmento de nuestros documentos. A continuaci√≥n, se presenta un ejemplo de c√≥mo se extraen y representan estos metadatos:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'xpath': '/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:LeaseParties',\n",
    "    'id': 'v1bvgaozfkak',\n",
    "    'source': 'TruTone Lane 2.docx',\n",
    "    'structure': 'p',\n",
    "    'tag': 'LeaseParties',\n",
    "    'Lease Date': 'April 24 \\n\\n ,',\n",
    "    'Landlord': 'BUBBA CENTER PARTNERSHIP',\n",
    "    'Tenant': 'Truetone Lane LLC',\n",
    "    'Lease Parties': 'Este ACUERDO DE ARRENDAMIENTO DE OFICINA (el \"Contrato\") es celebrado por y entre BUBBA CENTER PARTNERSHIP (\"Arrendador\"), y Truetone Lane LLC, una compa√±√≠a de responsabilidad limitada de Delaware (\"Arrendatario\").'\n",
    "}\n",
    "```\n",
    "\n",
    "Los metadatos adicionales, como los mostrados arriba, pueden ser extremadamente √∫tiles cuando se implementan `self-retrievers`, los cuales ser√°n explorados adetalle m√°s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar tus documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prefieres utilizar tus propios documentos, puedes cargarlos a trav√©s de la interfaz gr√°fica de [Docugami](https://www.docugami.com/). Una vez cargados, necesitar√°s asignar cada uno a un `docset`. Un `docset` es un conjunto de documentos que presentan una estructura an√°loga. Por ejemplo, todos los contratos de arrendamiento comercial por lo general poseen estructuras similares, por lo que pueden ser agrupados en un √∫nico `docset`.\n",
    "\n",
    "Despu√©s de crear tu `docset`, los documentos cargados ser√°n procesados y estar√°n disponibles para su acceso mediante la API de `Docugami`.\n",
    "\n",
    "Para recuperar los `ids` de tus documentos y de sus correspondientes `docsets`, puedes ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "curl --header \"Authorization: Bearer {YOUR_DOCUGAMI_TOKEN}\" \\\n",
    "  https://api.docugami.com/v1preview1/documents\n",
    "```\n",
    "\n",
    "Este comando te facilitar√° el acceso a la informaci√≥n relevante, optimizando as√≠ la administraci√≥n y organizaci√≥n de tus documentos dentro de `Docugami`.\n",
    "\n",
    "Una vez hayas extra√≠do los `ids` de tus documentos o de los `docsets`, podr√°s emplearlos para acceder a la informaci√≥n de tus documentos utilizando el `DocugamiLoader` de `Langchain`. Esto te permitir√° manipular y gestionar tus documentos dentro de tu aplicaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DocugamiLoader(\n",
    "    docset_id=\"\",\n",
    "    document_ids=None,\n",
    "    file_paths=None,\n",
    ")\n",
    "\n",
    "papers_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_in_the_middle_paper_docs = [\n",
    "    doc for doc in papers_docs if doc.metadata[\"source\"] == \"\"\n",
    "]\n",
    "for doc in lost_in_the_middle_paper_docs:\n",
    "    print(doc.metadata[\"tag\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
