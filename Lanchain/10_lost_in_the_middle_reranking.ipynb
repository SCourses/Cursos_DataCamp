{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perdido en el medio: El problema con los contextos largos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Independientemente de la arquitectura de tu modelo, existe una degradaci√≥n sustancial del rendimiento cuando incluyes m√°s de 10 documentos recuperados. En resumen: Cuando los modelos deben acceder a informaci√≥n relevante en medio de contextos largos, tienden a ignorar los documentos proporcionados. Ver: https://arxiv.org/abs/2307.03172\n",
    "\n",
    "Para evitar este problema, puedes reordenar los documentos despu√©s de recuperarlos para evitar la degradaci√≥n del rendimiento.\"\n",
    "\n",
    "Por: [Langchain](https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder)\n",
    "\n",
    "![Lost in the Middle](./diagrams/slide_diagrama_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=350,\n",
    "    chunk_overlap=10,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "\n",
    "documents = LangchainDocsLoader().load()\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n de retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Chroma.from_documents(documents[:100], embedding=OpenAIEmbeddings()).as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,\n",
    "        \"fetch_k\": 50,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f9dc437eb50>, search_type='mmr', search_kwargs={'k': 10, 'fetch_k': 50})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta con el retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Cookbook\\n\\nExample code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the [Prompt + LLM](/docs/expression_language/cookbook/prompt_llm_parser) page is a good place to start.\", metadata={'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='| Deprecated | Alternative | Reason |\\n| ---- | ---- | ---- |\\n| ChatVectorDBChain | ConversationalRetrievalChain | More general to all retrievers |\\n| create_ernie_fn_chain | create_ernie_fn_runnable | Use LCEL under the hood |\\n| created_structured_output_chain | create_structured_output_runnable | Use LCEL under the hood |\\n| NatBotChain |  | Not used |\\n| create_openai_fn_chain | create_openai_fn_runnable | Use LCEL under the hood |\\n| create_structured_output_chain | create_structured_output_runnable | Use LCEL under the hood |\\n| load_query_constructor_chain | load_query_constructor_runnable | Use LCEL under the hood |\\n| VectorDBQA | RetrievalQA | More general to all retrievers |\\n| Sequential Chain | LCEL | Obviated by LCEL |\\n| SimpleSequentialChain | LCEL | Obviated by LCEL |\\n| TransformChain | LCEL/RunnableLambda | Obviated by LCEL |\\n| create_tagging_chain | create_structured_output_runnable | Use LCEL under the hood |\\n| ChatAgent | create_react_agent | Use LCEL builder over a class |\\n| ConversationalAgent | create_react_agent | Use LCEL builder over a class |\\n| ConversationalChatAgent | create_json_chat_agent | Use LCEL builder over a class |\\n| initialize_agent | Individual create agent methods | Individual create agent methods are more clear |\\n| ZeroShotAgent | create_react_agent | Use LCEL builder over a class |', metadata={'description': '0.1.0 (Jan 5, 2024)', 'language': 'en', 'source': 'https://python.langchain.com/docs/changelog/langchain', 'title': 'langchain | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='| OpenAIFunctionsAgent | create_openai_functions_agent | Use LCEL builder over a class |\\n| OpenAIMultiFunctionsAgent | create_openai_tools_agent | Use LCEL builder over a class |\\n| SelfAskWithSearchAgent | create_self_ask_with_search | Use LCEL builder over a class |\\n| StructuredChatAgent | create_structured_chat_agent | Use LCEL builder over a class |\\n| XMLAgent | create_xml_agent | Use LCEL builder over a class |', metadata={'description': '0.1.0 (Jan 5, 2024)', 'language': 'en', 'source': 'https://python.langchain.com/docs/changelog/langchain', 'title': 'langchain | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='**Retries and fallbacks**\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\\n\\n**Access intermediate results**\\nFor more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every [LangServe](/docs/langserve) server.\\n\\n**Input and output schemas**\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\\n\\n**Seamless LangSmith tracing integration**\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](/docs/langsmith/) for maximum observability and debuggability.\\n\\n**Seamless LangServe deployment integration**\\nAny chain created with LCEL can be easily deployed using [LangServe](/docs/langserve).', metadata={'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='- ‚õì [How to use BGE Embeddings for LangChain](https://youtu.be/sWRvSG7vL4g?si=85jnvnmTCF9YIWXI)\\n- ‚õì [How to use Custom Prompts for RetrievalQA on LLaMA-2 7B](https://youtu.be/PDwUKves9GY?si=sMF99TWU0p4eiK80)', metadata={'description': 'Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.', 'language': 'en', 'source': 'https://python.langchain.com/docs/additional_resources/tutorials', 'title': 'Tutorials | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='| Notebook | Description |\\n| ---- | ---- |\\n| LLaMA2_sql_chat.ipynb | Build a chat application that interacts with a SQL database using an open source llm (llama2), specifically demonstrated on an SQLite database containing rosters. |\\n| Semi_Structured_RAG.ipynb | Perform retrieval-augmented generation (rag) on documents with semi-structured data, including text and tables, using unstructured for parsing, multi-vector retriever for storing, and lcel for implementing chains. |\\n| Semi_structured_and_multi_moda... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using unstructured for parsing, multi-vector retriever for storage and retrieval, and lcel for implementing chains. |\\n| Semi_structured_multi_modal_RA... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using various tools and methods such as unstructured for parsing, multi-vector retriever for storing, lcel for implementing chains, and open source language models like llama2, llava, and gpt4all. |\\n| analyze_document.ipynb | Analyze a single long document. |\\n| autogpt/autogpt.ipynb | Implement autogpt, a language model, with langchain primitives such as llms, prompttemplates, vectorstores, embeddings, and tools. |\\n| autogpt/marathon_times.ipynb | Implement autogpt for finding winning marathon times. |', metadata={'description': 'Example code for building applications with LangChain, with an emphasis on more applied and end-to-end examples than contained in the main documentation.', 'language': 'en', 'source': 'https://python.langchain.com/cookbook', 'title': 'LangChain cookbook | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# langchain-core\\n\\n## 0.1.7 (Jan 5, 2024)\\u200b\\n\\n#### Deleted\\u200b\\n\\nNo deletions.\\n\\n#### Deprecated\\u200b\\n\\n- `BaseChatModel` methods `__call__`, `call_as_llm`, `predict`, `predict_messages`. Will be removed in 0.2.0. Use `BaseChatModel.invoke` instead.\\n- `BaseChatModel` methods `apredict`, `apredict_messages`. Will be removed in 0.2.0. Use `BaseChatModel.ainvoke` instead.\\n- `BaseLLM` methods `__call__, `predict`, `predict_messages`. Will be removed in 0.2.0. Use `BaseLLM.invoke` instead.\\n- `BaseLLM` methods `apredict`, `apredict_messages`. Will be removed in 0.2.0. Use `BaseLLM.ainvoke` instead.\\n\\n#### Fixed\\u200b\\n\\n- Restrict recursive URL scraping: [#15559](https://github.com/langchain-ai/langchain/pull/15559)\\n\\n#### Added\\u200b\\n\\nNo additions.\\n\\n#### Beta\\u200b\\n\\n- Marked `langchain_core.load.load` and `langchain_core.load.loads` as beta.\\n- Marked `langchain_core.beta.runnables.context.ContextGet` and `langchain_core.beta.runnables.context.ContextSet` as beta.', metadata={'description': '0.1.7 (Jan 5, 2024)', 'language': 'en', 'source': 'https://python.langchain.com/docs/changelog/core', 'title': 'langchain-core | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='When running tests in a CI/CD pipeline, you may not want to modify the existing\\ncassettes. You can use the --vcr-record=none command-line option to disable recording\\nnew cassettes. Here\\'s an example:\\n\\n```bash\\npytest --log-cli-level=10 tests/integration_tests/vectorstores/test_pinecone.py --vcr-record=none\\npytest tests/integration_tests/vectorstores/test_elasticsearch.py --vcr-record=none\\n\\n```\\n\\n### Run some tests with coverage:\\u200b\\n\\n```bash\\npytest tests/integration_tests/vectorstores/test_elasticsearch.py --cov=langchain --cov-report=html\\nstart \"\" htmlcov/index.html || open htmlcov/index.html\\n\\n```\\n\\n## Coverage\\u200b\\n\\nCode coverage (i.e. the amount of code that is covered by unit tests) helps identify areas of the code that are potentially more or less brittle.\\n\\nCoverage requires the dependencies for integration tests:\\n\\n```bash\\npoetry install --with test_integration\\n```\\n\\nTo get a report of current coverage, run the following:\\n\\n```bash\\nmake coverage\\n```', metadata={'description': 'All of our packages have unit tests and integration tests, and we favor unit tests over integration tests.', 'language': 'en', 'source': 'https://python.langchain.com/docs/contributing/testing', 'title': 'Testing | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"```bash\\ncd libs/partners\\nlangchain-cli integration new\\n> Name: parrot-link\\n> Name of integration in PascalCase [ParrotLink]: ParrotLink\\n```\\n\\nThis will create a new package in `libs/partners/parrot-link` with the following structure:\\n\\n```text\\nlibs/partners/parrot-link/\\n  langchain_parrot_link/ # folder containing your package\\n    ...\\n  tests/\\n    ...\\n  docs/ # bootstrapped docs notebooks, must be moved to /docs in monorepo root\\n    ...\\n  scripts/ # scripts for CI\\n    ...\\n  LICENSE\\n  README.md # fill out with information about your package\\n  Makefile # default commands for CI\\n  pyproject.toml # package metadata, mostly managed by Poetry\\n  poetry.lock # package lockfile, managed by Poetry\\n  .gitignore\\n```\\n\\n### Implement your package\\u200b\\n\\nFirst, add any dependencies your package needs, such as your company's SDK:\\n\\n```bash\\npoetry add parrot-link-sdk\\n```\\n\\nIf you need separate dependencies for type checking, you can add them to the `typing` group with:\\n\\n```bash\\npoetry add --group typing types-parrot-link-sdk\\n```\\n\\nThen, implement your package in `libs/partners/parrot-link/langchain_parrot_link`.\\n\\nBy default, this will include stubs for a Chat Model, an LLM, and/or a Vector Store. You should delete any of the files you won't use and remove them from `__init__.py`.\\n\\n### Write Unit and Integration Tests\\u200b\", metadata={'description': 'To begin, make sure you have all the dependencies outlined in guide on Contributing Code.', 'language': 'en', 'source': 'https://python.langchain.com/docs/contributing/integrations', 'title': 'Contribute Integrations | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# Agents\\n\\nYou can pass a Runnable into an agent.\\n\\n```python\\nfrom langchain import hub\\nfrom langchain.agents import AgentExecutor, tool\\nfrom langchain.agents.output_parsers import XMLAgentOutputParser\\nfrom langchain_community.chat_models import ChatAnthropic\\n```\\n\\n```python\\nmodel = ChatAnthropic(model=\"claude-2\")\\n```\\n\\n```python\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search things about current events.\"\"\"\\n    return \"32 degrees\"\\n```\\n\\n```python\\ntool_list = [search]\\n```\\n\\n```python\\n# Get the prompt to use - you can modify this!\\nprompt = hub.pull(\"hwchase17/xml-agent-convo\")\\n```\\n\\n```python\\n# Logic for going from intermediate steps to a string to pass into model\\n# This is pretty tied to the prompt\\ndef convert_intermediate_steps(intermediate_steps):\\n    log = \"\"\\n    for action, observation in intermediate_steps:\\n        log += (\\n            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\\n            f\"</tool_input><observation>{observation}</observation>\"\\n        )\\n    return log\\n\\n# Logic for converting tools to string to go in prompt\\ndef convert_tools(tools):\\n    return \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n```\\n\\nBuilding an agent from a runnable usually involves a few things:\\n\\n1. Data processing for the intermediate steps. These need to\\nrepresented in a way that the language model can recognize them.\\nThis should be pretty tightly coupled to the instructions in the\\nprompt\\n\\n2. The prompt itself\\n\\n3. The model, complete with stop tokens if needed', metadata={'description': 'You can pass a Runnable into an agent.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/agent', 'title': 'Agents | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs = retriever.get_relevant_documents(\n",
    "    \"How to use LCEL ainvoke with a retriever?\"\n",
    ")\n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordenado de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='| Deprecated | Alternative | Reason |\\n| ---- | ---- | ---- |\\n| ChatVectorDBChain | ConversationalRetrievalChain | More general to all retrievers |\\n| create_ernie_fn_chain | create_ernie_fn_runnable | Use LCEL under the hood |\\n| created_structured_output_chain | create_structured_output_runnable | Use LCEL under the hood |\\n| NatBotChain |  | Not used |\\n| create_openai_fn_chain | create_openai_fn_runnable | Use LCEL under the hood |\\n| create_structured_output_chain | create_structured_output_runnable | Use LCEL under the hood |\\n| load_query_constructor_chain | load_query_constructor_runnable | Use LCEL under the hood |\\n| VectorDBQA | RetrievalQA | More general to all retrievers |\\n| Sequential Chain | LCEL | Obviated by LCEL |\\n| SimpleSequentialChain | LCEL | Obviated by LCEL |\\n| TransformChain | LCEL/RunnableLambda | Obviated by LCEL |\\n| create_tagging_chain | create_structured_output_runnable | Use LCEL under the hood |\\n| ChatAgent | create_react_agent | Use LCEL builder over a class |\\n| ConversationalAgent | create_react_agent | Use LCEL builder over a class |\\n| ConversationalChatAgent | create_json_chat_agent | Use LCEL builder over a class |\\n| initialize_agent | Individual create agent methods | Individual create agent methods are more clear |\\n| ZeroShotAgent | create_react_agent | Use LCEL builder over a class |', metadata={'description': '0.1.0 (Jan 5, 2024)', 'language': 'en', 'source': 'https://python.langchain.com/docs/changelog/langchain', 'title': 'langchain | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='**Retries and fallbacks**\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\\n\\n**Access intermediate results**\\nFor more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every [LangServe](/docs/langserve) server.\\n\\n**Input and output schemas**\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\\n\\n**Seamless LangSmith tracing integration**\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](/docs/langsmith/) for maximum observability and debuggability.\\n\\n**Seamless LangServe deployment integration**\\nAny chain created with LCEL can be easily deployed using [LangServe](/docs/langserve).', metadata={'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='| Notebook | Description |\\n| ---- | ---- |\\n| LLaMA2_sql_chat.ipynb | Build a chat application that interacts with a SQL database using an open source llm (llama2), specifically demonstrated on an SQLite database containing rosters. |\\n| Semi_Structured_RAG.ipynb | Perform retrieval-augmented generation (rag) on documents with semi-structured data, including text and tables, using unstructured for parsing, multi-vector retriever for storing, and lcel for implementing chains. |\\n| Semi_structured_and_multi_moda... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using unstructured for parsing, multi-vector retriever for storage and retrieval, and lcel for implementing chains. |\\n| Semi_structured_multi_modal_RA... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using various tools and methods such as unstructured for parsing, multi-vector retriever for storing, lcel for implementing chains, and open source language models like llama2, llava, and gpt4all. |\\n| analyze_document.ipynb | Analyze a single long document. |\\n| autogpt/autogpt.ipynb | Implement autogpt, a language model, with langchain primitives such as llms, prompttemplates, vectorstores, embeddings, and tools. |\\n| autogpt/marathon_times.ipynb | Implement autogpt for finding winning marathon times. |', metadata={'description': 'Example code for building applications with LangChain, with an emphasis on more applied and end-to-end examples than contained in the main documentation.', 'language': 'en', 'source': 'https://python.langchain.com/cookbook', 'title': 'LangChain cookbook | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='When running tests in a CI/CD pipeline, you may not want to modify the existing\\ncassettes. You can use the --vcr-record=none command-line option to disable recording\\nnew cassettes. Here\\'s an example:\\n\\n```bash\\npytest --log-cli-level=10 tests/integration_tests/vectorstores/test_pinecone.py --vcr-record=none\\npytest tests/integration_tests/vectorstores/test_elasticsearch.py --vcr-record=none\\n\\n```\\n\\n### Run some tests with coverage:\\u200b\\n\\n```bash\\npytest tests/integration_tests/vectorstores/test_elasticsearch.py --cov=langchain --cov-report=html\\nstart \"\" htmlcov/index.html || open htmlcov/index.html\\n\\n```\\n\\n## Coverage\\u200b\\n\\nCode coverage (i.e. the amount of code that is covered by unit tests) helps identify areas of the code that are potentially more or less brittle.\\n\\nCoverage requires the dependencies for integration tests:\\n\\n```bash\\npoetry install --with test_integration\\n```\\n\\nTo get a report of current coverage, run the following:\\n\\n```bash\\nmake coverage\\n```', metadata={'description': 'All of our packages have unit tests and integration tests, and we favor unit tests over integration tests.', 'language': 'en', 'source': 'https://python.langchain.com/docs/contributing/testing', 'title': 'Testing | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# Agents\\n\\nYou can pass a Runnable into an agent.\\n\\n```python\\nfrom langchain import hub\\nfrom langchain.agents import AgentExecutor, tool\\nfrom langchain.agents.output_parsers import XMLAgentOutputParser\\nfrom langchain_community.chat_models import ChatAnthropic\\n```\\n\\n```python\\nmodel = ChatAnthropic(model=\"claude-2\")\\n```\\n\\n```python\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Search things about current events.\"\"\"\\n    return \"32 degrees\"\\n```\\n\\n```python\\ntool_list = [search]\\n```\\n\\n```python\\n# Get the prompt to use - you can modify this!\\nprompt = hub.pull(\"hwchase17/xml-agent-convo\")\\n```\\n\\n```python\\n# Logic for going from intermediate steps to a string to pass into model\\n# This is pretty tied to the prompt\\ndef convert_intermediate_steps(intermediate_steps):\\n    log = \"\"\\n    for action, observation in intermediate_steps:\\n        log += (\\n            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\\n            f\"</tool_input><observation>{observation}</observation>\"\\n        )\\n    return log\\n\\n# Logic for converting tools to string to go in prompt\\ndef convert_tools(tools):\\n    return \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n```\\n\\nBuilding an agent from a runnable usually involves a few things:\\n\\n1. Data processing for the intermediate steps. These need to\\nrepresented in a way that the language model can recognize them.\\nThis should be pretty tightly coupled to the instructions in the\\nprompt\\n\\n2. The prompt itself\\n\\n3. The model, complete with stop tokens if needed', metadata={'description': 'You can pass a Runnable into an agent.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/agent', 'title': 'Agents | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"```bash\\ncd libs/partners\\nlangchain-cli integration new\\n> Name: parrot-link\\n> Name of integration in PascalCase [ParrotLink]: ParrotLink\\n```\\n\\nThis will create a new package in `libs/partners/parrot-link` with the following structure:\\n\\n```text\\nlibs/partners/parrot-link/\\n  langchain_parrot_link/ # folder containing your package\\n    ...\\n  tests/\\n    ...\\n  docs/ # bootstrapped docs notebooks, must be moved to /docs in monorepo root\\n    ...\\n  scripts/ # scripts for CI\\n    ...\\n  LICENSE\\n  README.md # fill out with information about your package\\n  Makefile # default commands for CI\\n  pyproject.toml # package metadata, mostly managed by Poetry\\n  poetry.lock # package lockfile, managed by Poetry\\n  .gitignore\\n```\\n\\n### Implement your package\\u200b\\n\\nFirst, add any dependencies your package needs, such as your company's SDK:\\n\\n```bash\\npoetry add parrot-link-sdk\\n```\\n\\nIf you need separate dependencies for type checking, you can add them to the `typing` group with:\\n\\n```bash\\npoetry add --group typing types-parrot-link-sdk\\n```\\n\\nThen, implement your package in `libs/partners/parrot-link/langchain_parrot_link`.\\n\\nBy default, this will include stubs for a Chat Model, an LLM, and/or a Vector Store. You should delete any of the files you won't use and remove them from `__init__.py`.\\n\\n### Write Unit and Integration Tests\\u200b\", metadata={'description': 'To begin, make sure you have all the dependencies outlined in guide on Contributing Code.', 'language': 'en', 'source': 'https://python.langchain.com/docs/contributing/integrations', 'title': 'Contribute Integrations | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# langchain-core\\n\\n## 0.1.7 (Jan 5, 2024)\\u200b\\n\\n#### Deleted\\u200b\\n\\nNo deletions.\\n\\n#### Deprecated\\u200b\\n\\n- `BaseChatModel` methods `__call__`, `call_as_llm`, `predict`, `predict_messages`. Will be removed in 0.2.0. Use `BaseChatModel.invoke` instead.\\n- `BaseChatModel` methods `apredict`, `apredict_messages`. Will be removed in 0.2.0. Use `BaseChatModel.ainvoke` instead.\\n- `BaseLLM` methods `__call__, `predict`, `predict_messages`. Will be removed in 0.2.0. Use `BaseLLM.invoke` instead.\\n- `BaseLLM` methods `apredict`, `apredict_messages`. Will be removed in 0.2.0. Use `BaseLLM.ainvoke` instead.\\n\\n#### Fixed\\u200b\\n\\n- Restrict recursive URL scraping: [#15559](https://github.com/langchain-ai/langchain/pull/15559)\\n\\n#### Added\\u200b\\n\\nNo additions.\\n\\n#### Beta\\u200b\\n\\n- Marked `langchain_core.load.load` and `langchain_core.load.loads` as beta.\\n- Marked `langchain_core.beta.runnables.context.ContextGet` and `langchain_core.beta.runnables.context.ContextSet` as beta.', metadata={'description': '0.1.7 (Jan 5, 2024)', 'language': 'en', 'source': 'https://python.langchain.com/docs/changelog/core', 'title': 'langchain-core | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='- ‚õì [How to use BGE Embeddings for LangChain](https://youtu.be/sWRvSG7vL4g?si=85jnvnmTCF9YIWXI)\\n- ‚õì [How to use Custom Prompts for RetrievalQA on LLaMA-2 7B](https://youtu.be/PDwUKves9GY?si=sMF99TWU0p4eiK80)', metadata={'description': 'Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.', 'language': 'en', 'source': 'https://python.langchain.com/docs/additional_resources/tutorials', 'title': 'Tutorials | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='| OpenAIFunctionsAgent | create_openai_functions_agent | Use LCEL builder over a class |\\n| OpenAIMultiFunctionsAgent | create_openai_tools_agent | Use LCEL builder over a class |\\n| SelfAskWithSearchAgent | create_self_ask_with_search | Use LCEL builder over a class |\\n| StructuredChatAgent | create_structured_chat_agent | Use LCEL builder over a class |\\n| XMLAgent | create_xml_agent | Use LCEL builder over a class |', metadata={'description': '0.1.0 (Jan 5, 2024)', 'language': 'en', 'source': 'https://python.langchain.com/docs/changelog/langchain', 'title': 'langchain | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"# Cookbook\\n\\nExample code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the [Prompt + LLM](/docs/expression_language/cookbook/prompt_llm_parser) page is a good place to start.\", metadata={'description': \"Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.\", 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/', 'title': 'Cookbook | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = list(reordering.transform_documents(relevant_docs))\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del reordenador en nuestro pipeline de `Retrieval Augmented Generation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_documents(documents: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "                                      \n",
    "Answer the following question, if you don't know the answer, just write \"I don't know.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "stuff_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | reordering.transform_documents\n",
    "        | combine_documents,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a chain using LCEL, you need to define the steps of the chain and their order of execution. Each step in the chain is represented by a function or a class. Here are the general steps to create a chain using LCEL:\n",
      "\n",
      "1. Define the functions or classes that will be used as steps in the chain.\n",
      "2. Specify the order of execution for the steps.\n",
      "3. Use the pipe operator (`|`) to connect the steps together.\n",
      "4. Optionally, you can add additional functions or classes to modify the output of the chain.\n",
      "5. Invoke the chain with the input data.\n",
      "\n",
      "In the given text extracts, an example of creating a chain using LCEL can be seen in the following code snippet:\n",
      "\n",
      "```python\n",
      "chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run\n",
      "```\n",
      "\n",
      "In this example, the `prompt` is connected to the `model` using the pipe operator (`|`). The output of the `model` is then passed to the `StrOutputParser`, followed by `_sanitize_output`, and finally to `PythonREPL().run`. The `chain` is then invoked with the input data using the `invoke` method.\n",
      "\n",
      "Please note that this is just one example of creating a chain using LCEL, and the specific steps and functions/classes used may vary depending on the requirements of the chain.\n"
     ]
    }
   ],
   "source": [
    "response = stuff_chain.invoke(input={\"question\": \"How to create a chain using LCEL?\"}).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure OpenAI\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"06fe9b5f4d9846d19cb44f3d456c4dad\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]='2023-06-01-preview'\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"]='https://openai-cigo.openai.azure.com/'\n",
    "llm=AzureChatOpenAI(openai_api_key=\"06fe9b5f4d9846d19cb44f3d456c4dad\", deployment_name=\"Test\", model_name=\"gpt-3.5-turbo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¬°Hola! ¬øEn qu√© puedo ayudarte hoy?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "llm([HumanMessage(content='Hola')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
