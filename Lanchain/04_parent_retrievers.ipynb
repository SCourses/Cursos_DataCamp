{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al fragmentar documentos para su procesamiento y recuperaci√≥n, a menudo nos enfrentamos a un dilema: \n",
    "\n",
    "Por un lado, se podr√≠an preferir documentos m√°s reducidos, de modo que los `embeddings` puedan reflejar su significado de manera m√°s exacta y espec√≠fica. Cuando un documento es demasiado extenso, existe el riesgo de que los `embeddings` pierdan su significado y precisi√≥n.\n",
    "\n",
    "Por otro lado, es crucial mantener documentos con una longitud considerable para preservar el contexto de cada fragmento, y as√≠ garantizar la coherencia e integridad de la informaci√≥n.\n",
    "\n",
    "`ParentDocumentRetriever` aborda eficazmente esta contradicci√≥n al dividir y almacenar fragmentos de datos concisos. Durante el proceso de recuperaci√≥n, este sistema primero accede a los fragmentos m√°s peque√±os y posteriormente identifica y busca los identificadores principales de dichos fragmentos, retornando finalmente los documentos de mayor tama√±o. \n",
    "\n",
    "Es crucial aclarar que el t√©rmino \"documento principal\" hace referencia al documento fuente del que se extrajo un fragmento peque√±o. Esto puede ser el documento √≠ntegro original o un segmento m√°s amplio del mismo.\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Por ejemplo, si se est√° procesando un libro, podr√≠amos querer fragmentar cada cap√≠tulo o secci√≥n para obtener `embeddings` m√°s precisos sobre los temas tratados en cada uno. En este caso, un cap√≠tulo ser√≠a un \"documento principal\", y cada fragmento o secci√≥n del cap√≠tulo representar√≠a un fragmento m√°s peque√±o.\n",
    "\n",
    "1. **Proceso de Fragmentaci√≥n:**\n",
    "   - El libro se divide en cap√≠tulos.\n",
    "   - Cada cap√≠tulo se fragmenta en secciones m√°s peque√±as.\n",
    "\n",
    "2. **Proceso de Recuperaci√≥n:**\n",
    "   - `ParentDocumentRetriever` recupera primero las secciones m√°s peque√±as del cap√≠tulo.\n",
    "   - Luego, identifica y recupera el cap√≠tulo completo (documento principal) bas√°ndose en los fragmentos peque√±os.\n",
    "\n",
    "Este enfoque permite una b√∫squeda y recuperaci√≥n de informaci√≥n m√°s eficiente y precisa, asegurando que cada fragmento recuperado mantenga su contexto original y, al mismo tiempo, brinde un entendimiento profundo y detallado de su contenido.\n",
    "\n",
    "![Parent Retrievers](./diagrams/slide_diagrama_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from src.langchain_docs_loader import LangchainDocsLoader, num_tokens_from_string\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vectorstore = partial(\n",
    "    Chroma,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = LangchainDocsLoader()\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperaci√≥n de los documentos completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter=RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "\n",
    "vectorstore=get_vectorstore(collection_name=\"full_documents\")\n",
    "\n",
    "store=InMemoryStore()\n",
    "\n",
    "retriever=ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de documentos en nuestra `Store` es igual a la cantidad de documentos en nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al buscar documentos directamente en la `VectorStore`, obtendr√°s fragmentos de documentos que fueron procesados por el `TextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does the MultiQueryRetriever might be able to overcome some of the limitations of...?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the', metadata={'description': 'The Fleet AI team is on a mission to embed the world‚Äôs most important', 'doc_id': '09bcdad3-51ca-4f3f-9dbf-b7034270354f', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers/fleet_context', 'title': 'Fleet AI Libraries Context | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. ``` # Build a sample vectorDBfrom langchain_community.vectorstores import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import OpenAIEmbeddingsfrom', metadata={'description': 'The Fleet AI team is on a mission to embed the world‚Äôs most important', 'doc_id': '09bcdad3-51ca-4f3f-9dbf-b7034270354f', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers/fleet_context', 'title': 'Fleet AI Libraries Context | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content=\"potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain', 'type': None, 'url': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever'}),\", metadata={'description': 'The Fleet AI team is on a mission to embed the world‚Äôs most important', 'doc_id': '09bcdad3-51ca-4f3f-9dbf-b7034270354f', 'language': 'en', 'source': 'https://python.langchain.com/docs/integrations/retrievers/fleet_context', 'title': 'Fleet AI Libraries Context | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='able to overcome some of the limitations of the distance-based retrieval\\nand get a richer set of results.', metadata={'description': 'Distance-based vector database retrieval embeds (represents) queries in', 'doc_id': 'f8891f65-ccfb-4337-ba41-4474b23aedb7', 'language': 'en', 'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_documents_similarity = vectorstore.similarity_search(\n",
    "    query,\n",
    ")\n",
    "full_documents_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora realizas una b√∫squeda en el `ParentDocumentRetriever`, obtendr√°s los documentos completos.\n",
    "Esto se debe a que el `ParentDocumentRetriever` primero busca los fragmentos que hacen `match` con la `query`, despu√©s busca los documentos completos sin repeticiones y finalmente devuelve el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Fleet AI Libraries Context\\n\\nThe Fleet AI team is on a mission to embed the world‚Äôs most important\\ndata. They‚Äôve started by embedding the top 1200 Python libraries to\\nenable code generation with up-to-date knowledge. They‚Äôve been kind\\nenough to share their embeddings of the [LangChain\\ndocs](https://python.langchain.com/docs/get_started/introduction) and\\n[API\\nreference](https://api.python.langchain.com/en/latest/api_reference.html).\\n\\nLet‚Äôs take a look at how we can use these embeddings to power a docs\\nretrieval system and ultimately a simple code generating chain!\\n\\n```python\\n%pip install --upgrade --quiet  langchain fleet-context langchain-openai pandas faiss-cpu # faiss-gpu for CUDA supported GPU\\n```\\n\\n```python\\nfrom operator import itemgetter\\nfrom typing import Any, Optional, Type\\n\\nimport pandas as pd\\nfrom langchain.retrievers import MultiVectorRetriever\\nfrom langchain.schema import Document\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.stores import BaseStore\\nfrom langchain_core.vectorstores import VectorStore\\nfrom langchain_openai import OpenAIEmbeddings\\n\\ndef load_fleet_retriever(\\n    df: pd.DataFrame,\\n    *,\\n    vectorstore_cls: Type[VectorStore] = FAISS,\\n    docstore: Optional[BaseStore] = None,\\n    **kwargs: Any,\\n):\\n    vectorstore = _populate_vectorstore(df, vectorstore_cls)\\n    if docstore is None:\\n        return vectorstore.as_retriever(**kwargs)\\n    else:\\n        _populate_docstore(df, docstore)\\n        return MultiVectorRetriever(\\n            vectorstore=vectorstore, docstore=docstore, id_key=\"parent\", **kwargs\\n        )\\n\\ndef _populate_vectorstore(\\n    df: pd.DataFrame,\\n    vectorstore_cls: Type[VectorStore],\\n) -> VectorStore:\\n    if not hasattr(vectorstore_cls, \"from_embeddings\"):\\n        raise ValueError(\\n            f\"Incompatible vector store class {vectorstore_cls}.\"\\n            \"Must implement `from_embeddings` class method.\"\\n        )\\n    texts_embeddings = []\\n    metadatas = []\\n    for _, row in df.iterrows():\\n        texts_embeddings.append((row.metadata[\"text\"], row[\"dense_embeddings\"]))\\n        metadatas.append(row.metadata)\\n    return vectorstore_cls.from_embeddings(\\n        texts_embeddings,\\n        OpenAIEmbeddings(model=\"text-embedding-ada-002\"),\\n        metadatas=metadatas,\\n    )\\n\\ndef _populate_docstore(df: pd.DataFrame, docstore: BaseStore) -> None:\\n    parent_docs = []\\n    df = df.copy()\\n    df[\"parent\"] = df.metadata.apply(itemgetter(\"parent\"))\\n    for parent_id, group in df.groupby(\"parent\"):\\n        sorted_group = group.iloc[\\n            group.metadata.apply(itemgetter(\"section_index\")).argsort()\\n        ]\\n        text = \"\".join(sorted_group.metadata.apply(itemgetter(\"text\")))\\n        metadata = {\\n            k: sorted_group.iloc[0].metadata[k] for k in (\"title\", \"type\", \"url\")\\n        }\\n        text = metadata[\"title\"] + \"\\\\n\" + text\\n        metadata[\"id\"] = parent_id\\n        parent_docs.append(Document(page_content=text, metadata=metadata))\\n    docstore.mset(((d.metadata[\"id\"], d) for d in parent_docs))\\n```\\n\\n## Retriever chunks\\u200b\\n\\nAs part of their embedding process, the Fleet AI team first chunked long\\ndocuments before embedding them. This means the vectors correspond to\\nsections of pages in the LangChain docs, not entire pages. By default,\\nwhen we spin up a retriever from these embeddings, we‚Äôll be retrieving\\nthese embedded chunks.\\n\\nWe will be using Fleet Context‚Äôs `download_embeddings()` to grab\\nLangchain‚Äôs documentation embeddings. You can view all supported\\nlibraries‚Äô documentation at [https://fleet.so/context](https://fleet.so/context).\\n\\n```python\\nfrom context import download_embeddings\\n\\ndf = download_embeddings(\"langchain\")\\nvecstore_retriever = load_fleet_retriever(df)\\n```\\n\\n```python\\nvecstore_retriever.get_relevant_documents(\"How does the multi vector retriever work\")\\n```\\n\\n```text\\n[Document(page_content=\"# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\'s very easy to construct a retriever. Let\\'s walk through an example.\", metadata={\\'id\\': \\'f509f20d-4c63-4a5a-a40a-5c4c0f099839\\', \\'library_id\\': \\'4506492b-70de-49f1-ba2e-d65bd7048a28\\', \\'page_id\\': \\'d78cf422-2dab-4860-80fe-d71a3619b02f\\', \\'parent\\': \\'c153ebd9-2611-4a43-9db6-daa1f5f214f6\\', \\'section_id\\': \\'\\', \\'section_index\\': 0, \\'text\\': \"# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\'s very easy to construct a retriever. Let\\'s walk through an example.\", \\'title\\': \\'Vector store-backed retriever | ü¶úÔ∏èüîó Langchain\\', \\'type\\': None, \\'url\\': \\'https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\\'}),\\n Document(page_content=\\'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\\', metadata={\\'id\\': \\'e06e6bb5-127a-49f7-9511-247d279d0d83\\', \\'library_id\\': \\'4506492b-70de-49f1-ba2e-d65bd7048a28\\', \\'page_id\\': \\'7c7dd0de-25e0-4e1d-9237-cfd2674c29d4\\', \\'parent\\': \\'beec5531-16a7-453c-80ab-c5628e0236ce\\', \\'section_id\\': \\'\\', \\'section_index\\': 0, \\'text\\': \\'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\\', \\'title\\': \\'MultiVector Retriever | ü¶úÔ∏èüîó Langchain\\', \\'type\\': None, \\'url\\': \\'https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\\'}),\\n Document(page_content=\\'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\\', metadata={\\'id\\': \\'dc3b8e5b-a591-4a46-bfeb-a91b36affae1\\', \\'library_id\\': \\'4506492b-70de-49f1-ba2e-d65bd7048a28\\', \\'page_id\\': \\'31f80e84-c5db-4da2-939c-bccf519864a3\\', \\'parent\\': \\'f7c20633-6a60-4ca3-96b1-13fee66e321d\\', \\'section_id\\': \\'\\', \\'section_index\\': 0, \\'text\\': \\'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\\', \\'title\\': \\'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain\\', \\'type\\': None, \\'url\\': \\'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\\'}),\\n Document(page_content=\\'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[¬∂](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[¬∂](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[¬∂](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)\\', metadata={\\'id\\': \\'1f4ca702-35b8-44c0-b33b-18c09a1f787d\\', \\'library_id\\': \\'6254c672-7aa0-4233-b0a6-804bd273752b\\', \\'page_id\\': \\'87f5d1fb-a1c6-4080-9d2b-7b88794eb6bf\\', \\'parent\\': \\'1820c44d-7783-4846-a11c-106b18da015d\\', \\'section_id\\': \\'langchain-retrievers-multi-vector-multivectorretriever\\', \\'section_index\\': 0, \\'text\\': \\'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[¬∂](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[¬∂](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[¬∂](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)\\', \\'title\\': \\'langchain.retrievers.multi_vector.MultiVectorRetriever ‚Äî ü¶úüîó LangChain 0.0.322\\', \\'type\\': None, \\'url\\': \\'https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html#langchain-retrievers-multi-vector-multivectorretriever\\'})]\\n```\\n\\n## Other packages\\u200b\\n\\nYou can download and use other embeddings from [this Dropbox\\nlink](https://www.dropbox.com/scl/fo/54t2e7fogtixo58pnlyub/h?rlkey=tne16wkssgf01jor0p1iqg6p9&dl=0).\\n\\n## Retrieve parent docs\\u200b\\n\\nThe embeddings provided by Fleet AI contain metadata that indicates\\nwhich embedding chunks correspond to the same original document page. If\\nwe‚Äôd like we can use this information to retrieve whole parent\\ndocuments, and not just embedded chunks. Under the hood, we‚Äôll use a\\nMultiVectorRetriever and a BaseStore object to search for relevant\\nchunks and then map them to their parent document.\\n\\n```python\\nfrom langchain.storage import InMemoryStore\\n\\nparent_retriever = load_fleet_retriever(\\n    \"https://www.dropbox.com/scl/fi/4rescpkrg9970s3huz47l/libraries_langchain_release.parquet?rlkey=283knw4wamezfwiidgpgptkep&dl=1\",\\n    docstore=InMemoryStore(),\\n)\\n```\\n\\n```python\\nparent_retriever.get_relevant_documents(\"How does the multi vector retriever work\")\\n```\\n\\n```text\\n[Document(page_content=\\'Vector store-backed retriever | ü¶úÔ∏èüîó Langchain\\\\n# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\\\\\'s very easy to construct a retriever. Let\\\\\\'s walk through an example.Once you construct a vector store, it\\\\\\'s very easy to construct a retriever. Let\\\\\\'s walk through an example. ``` from langchain_community.document_loaders import TextLoaderloader = TextLoader(\\\\\\'../../../state_of_the_union.txt\\\\\\') ``` ``` from langchain.text_splitter import CharacterTextSplitterfrom langchain_community.vectorstores import FAISSfrom langchain_openai import OpenAIEmbeddingsdocuments = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()db = FAISS.from_documents(texts, embeddings) ``` ``` Exiting: Cleaning up .chroma directory ``` ``` retriever = db.as_retriever() ``` ``` docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ## Maximum marginal relevance retrieval[\\\\u200b](#maximum-marginal-relevance-retrieval) By default, the vector store retriever uses similarity search.If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ``` retriever = db.as_retriever(search_type=\"mmr\") ``` ``` docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ## Similarity score threshold retrieval[\\\\u200b](#similarity-score-threshold-retrieval) You can also a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ``` retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5}) ``` ``` docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ## Specifying top k[\\\\u200b](#specifying-top-k) You can also specify search kwargs like `k` to use when doing retrieval.``` retriever = db.as_retriever(search_kwargs={\"k\": 1}) ``` ``` docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ``` len(docs) ``` ``` 1 ```\\', metadata={\\'title\\': \\'Vector store-backed retriever | ü¶úÔ∏èüîó Langchain\\', \\'type\\': None, \\'url\\': \\'https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\\', \\'id\\': \\'c153ebd9-2611-4a43-9db6-daa1f5f214f6\\'}),\\n Document(page_content=\\'MultiVector Retriever | ü¶úÔ∏èüîó Langchain\\\\n# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control. ``` from langchain.retrievers.multi_vector import MultiVectorRetriever ``` ``` from langchain_community.vectorstores import Chromafrom langchain_openai import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.storage import InMemoryStorefrom langchain_community.document_loaders import TextLoader ``` ``` loaders = [ TextLoader(\\\\\\'../../paul_graham_essay.txt\\\\\\'), TextLoader(\\\\\\'../../state_of_the_union.txt\\\\\\'),]docs = []for l in loaders: docs.extend(l.load())text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)docs = text_splitter.split_documents(docs) ``` ## Smaller chunks[\\\\u200b](#smaller-chunks) Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the `ParentDocumentRetriever` does. Here we show what is going on under the hood.``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = \"doc_id\"# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)import uuiddoc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` # The splitter to use to create smaller chunkschild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) ``` ``` sub_docs = []for i, doc in enumerate(docs): _id = doc_ids[i] _sub_docs = child_text_splitter.split_documents([doc]) for _doc in _sub_docs: _doc.metadata[id_key] = _id sub_docs.extend(_sub_docs) ``` ``` retriever.vectorstore.add_documents(sub_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # Vectorstore alone retrieves the small chunksretriever.vectorstore.similarity_search(\"justice breyer\")[0] ``` ``` Document(page_content=\\\\\\'Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.Justice Breyer, thank you for your service. \\\\\\\\n\\\\\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\\\', metadata={\\\\\\'doc_id\\\\\\': \\\\\\'10e9cbc0-4ba5-4d79-a09b-c033d1ba7b01\\\\\\', \\\\\\'source\\\\\\': \\\\\\'../../state_of_the_union.txt\\\\\\'}) ``` ``` # Retriever returns larger chunkslen(retriever.get_relevant_documents(\"justice breyer\")[0].page_content) ``` ``` 9874 ``` ## Summary[\\\\u200b](#summary) Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.``` from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserimport uuidfrom langchain_core.documents import Document ``` ``` chain = ( {\"doc\": lambda x: x.page_content} | ChatPromptTemplate.from_template(\"Summarize the following document:\\\\\\\\n\\\\\\\\n{doc}\") | ChatOpenAI(max_retries=0) | StrOutputParser()) ``` ``` summaries = chain.batch(docs, {\"max_concurrency\": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = \"doc_id\"# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` summary_docs = [Document(page_content=s,metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)] ``` ``` retriever.vectorstore.add_documents(summary_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # # We can also add the original chunks to the vectorstore if we so want# for i, doc in enumerate(docs):# doc.metadata[id_key] = doc_ids[i]# retriever.vectorstore.add_documents(docs) ``` ``` sub_docs = vectorstore.similarity_search(\"justice breyer\") ``` ``` sub_docs[0] ``` ``` Document(page_content=\"The document is a transcript of a speech given by the President of the United States.The President discusses several important issues and initiatives, including the nomination of a Supreme Court Justice, border security and immigration reform, protecting women\\\\\\'s rights, advancing LGBTQ+ equality, bipartisan legislation, addressing the opioid epidemic and mental health, supporting veterans, investigating the health effects of burn pits on military personnel, ending cancer, and the strength and resilience of the American people. \", metadata={\\\\\\'doc_id\\\\\\': \\\\\\'79fa2e9f-28d9-4372-8af3-2caf4f1de312\\\\\\'}) ``` ``` retrieved_docs = retriever.get_relevant_documents(\"justice breyer\") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ``` ## Hypothetical Queries[\\\\u200b](#hypothetical-queries) An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.These questions can then be embedded ``` functions = [ { \"name\": \"hypothetical_questions\", \"description\": \"Generate hypothetical questions\", \"parameters\": { \"type\": \"object\", \"properties\": { \"questions\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, }, }, \"required\": [\"questions\"] } } ] ``` ``` from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = ( {\"doc\": lambda x: x.page_content} # Only asking for 3 hypothetical questions, but this could be adjusted | ChatPromptTemplate.from_template(\"Generate a list of 3 hypothetical questions that the below document could be used to answer:\\\\\\\\n\\\\\\\\n{doc}\") | ChatOpenAI(max_retries=0, model=\"gpt-4\").bind(functions=functions, function_call={\"name\": \"hypothetical_questions\"}) | JsonKeyOutputFunctionsParser(key_name=\"questions\")) ``` ``` chain.invoke(docs[0]) ``` ``` [\"What was the author\\\\\\'s initial impression of philosophy as a field of study, and how did it change when they got to college?\", \\\\\\'Why did the author decide to switch their focus to Artificial Intelligence (AI)? \\\\\\', \"What led to the author\\\\\\'s disillusionment with the field of AI as it was practiced at the time?\"]``` ``` hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = \"doc_id\"# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` question_docs = []for i, question_list in enumerate(hypothetical_questions): question_docs.extend([Document(page_content=s,metadata={id_key: doc_ids[i]}) for s in question_list]) ``` ``` retriever.vectorstore.add_documents(question_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` sub_docs = vectorstore.similarity_search(\"justice breyer\") ``` ``` sub_docs ``` ``` [Document(page_content=\"What is the President\\\\\\'s stance on immigration reform?\", metadata={\\\\\\'doc_id\\\\\\': \\\\\\'505d73e3-8350-46ec-a58e-3af032f04ab3\\\\\\'}), Document(page_content=\"What is the President\\\\\\'s stance on immigration reform? \", metadata={\\\\\\'doc_id\\\\\\': \\\\\\'1c9618f0-7660-4b4f-a37c-509cbbbf6dba\\\\\\'}), Document(page_content=\"What is the President\\\\\\'s stance on immigration reform? \", metadata={\\\\\\'doc_id\\\\\\': \\\\\\'82c08209-b904-46a8-9532-edd2380950b7\\\\\\'}), Document(page_content=\\\\\\'What measures is the President proposing to protect the rights of LGBTQ+ Americans? \\\\\\', metadata={\\\\\\'doc_id\\\\\\': \\\\\\'82c08209-b904-46a8-9532-edd2380950b7\\\\\\'})] ``` ``` retrieved_docs = retriever.get_relevant_documents(\"justice breyer\") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ```\\', metadata={\\'title\\': \\'MultiVector Retriever | ü¶úÔ∏èüîó Langchain\\', \\'type\\': None, \\'url\\': \\'https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\\', \\'id\\': \\'beec5531-16a7-453c-80ab-c5628e0236ce\\'}),\\n Document(page_content=\\'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain\\\\n# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. ``` # Build a sample vectorDBfrom langchain_community.vectorstores import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitter# Load blog postloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")data = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)splits = text_splitter.split_documents(data)# VectorDBembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=splits, embedding=embedding) ``` #### Simple usage[\\\\u200b](#simple-usage) Specify the LLM to use for query generation, and the retriever will do the rest.``` from langchain_openai import ChatOpenAIfrom langchain.retrievers.multi_query import MultiQueryRetrieverquestion = \"What are the approaches to Task Decomposition? \"llm = ChatOpenAI(temperature=0)retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm) ``` ``` # Set logging for the queriesimport logginglogging.basicConfig()logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO) ``` ``` unique_docs = retriever_from_llm.get_relevant_documents(query=question)len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\\\\\\'1. How can Task Decomposition be approached? \\\\\\', \\\\\\'2. What are the different methods for Task Decomposition? \\\\\\', \\\\\\'3. What are the various approaches to decomposing tasks?\\\\\\'] 5 ``` #### Supplying your own prompt[\\\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries.5 ``` #### Supplying your own prompt[\\\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries. ``` from typing import Listfrom langchain.chains import LLMChainfrom pydantic import BaseModel, Fieldfrom langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParser# Output parser will split the LLM result into a list of queriesclass LineList(BaseModel): # \"lines\" is the key (attribute name) of the parsed output lines: List[str] = Field(description=\"Lines of text\")class LineListOutputParser(PydanticOutputParser): def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = text.strip().split(\"\\\\\\\\n\") return LineList(lines=lines)output_parser = LineListOutputParser()QUERY_PROMPT = PromptTemplate( input_variables=[\"question\"], template=\"\"\"You are an AI language model assistant.Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\"\"\",)llm = ChatOpenAI(temperature=0)# Chainllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)# Other inputsquestion = \"What are the approaches to Task Decomposition?\" ``` ``` # Runretriever = MultiQueryRetriever( retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\") # \"lines\" is the key (attribute name) of the parsed output# Resultsunique_docs = retriever.get_relevant_documents( query=\"What does the course say about regression? \")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\"1.\")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What is the course\\\\\\'s perspective on regression? \", \\\\\\'2. Can you provide information on regression as discussed in the course? \\\\\\', \\\\\\'3. How does the course cover the topic of regression? \\\\\\', \"4. What are the course\\\\\\'s teachings on regression? \", \\\\\\'5. In relation to the course, what is mentioned about regression?\\\\\\'] 11 ```\\', metadata={\\'title\\': \\'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain\\', \\'type\\': None, \\'url\\': \\'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\\', \\'id\\': \\'f7c20633-6a60-4ca3-96b1-13fee66e321d\\'}),\\n Document(page_content=\\'langchain.retrievers.multi_vector.MultiVectorRetriever ‚Äî ü¶úüîó LangChain 0.0.322\\\\n# `langchain.retrievers.multi_vector`.MultiVectorRetriever[¬∂](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[¬∂](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[¬∂](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)\\', metadata={\\'title\\': \\'langchain.retrievers.multi_vector.MultiVectorRetriever ‚Äî ü¶úüîó LangChain 0.0.322\\', \\'type\\': None, \\'url\\': \\'https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html#langchain-retrievers-multi-vector-multivectorretriever\\', \\'id\\': \\'1820c44d-7783-4846-a11c-106b18da015d\\'})]\\n```\\n\\n## Putting it in a chain\\u200b\\n\\nLet‚Äôs try using our retrieval systems in a simple chain!\\n\\n```python\\nfrom langchain.schema import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\\n            \"system\",\\n            \"\"\"You are a great software engineer who is very familiar \\\\\\nwith Python. Given a user question or request about a new Python library called LangChain and \\\\\\nparts of the LangChain documentation, answer the question or generate the requested code. \\\\\\nYour answers must be accurate, should include code whenever possible, and should assume anything \\\\\\nabout LangChain which is note explicitly stated in the LangChain documentation. If the required \\\\\\ninformation is not available, just say so.\\n\\nLangChain Documentation\\n------------------\\n\\n{context}\"\"\",\\n        ),\\n        (\"human\", \"{question}\"),\\n    ]\\n)\\n\\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\\n\\nchain = (\\n    {\\n        \"question\": RunnablePassthrough(),\\n        \"context\": parent_retriever\\n        | (lambda docs: \"\\\\n\\\\n\".join(d.page_content for d in docs)),\\n    }\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nfor chunk in chain.invoke(\\n    \"How do I create a FAISS vector store retriever that returns 10 documents per search query\"\\n):\\n    print(chunk, end=\"\", flush=True)\\n```\\n\\n```text\\nTo create a FAISS vector store retriever that returns 10 documents per search query, you can use the following code:\\n\\n```python\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_community.vectorstores import FAISS\\n\\n# Assuming you have already loaded and split your documents\\n# into `texts` and initialized your `embeddings` object\\n\\n# Create the FAISS vector store\\ndb = FAISS.from_documents(texts, embeddings)\\n\\n# Create the retriever with the desired search kwargs\\nretriever = db.as_retriever(search_kwargs={\"k\": 10})\\n```\\n\\nNow, you can use the `retriever` object to get relevant documents using the `get_relevant_documents` method. For example:\\n\\n```python\\ndocs = retriever.get_relevant_documents(\"your search query\")\\n```\\n\\nThis will return a list of 10 documents that are most relevant to the given search query.\\n```', metadata={'source': 'https://python.langchain.com/docs/integrations/retrievers/fleet_context', 'title': 'Fleet AI Libraries Context | ü¶úÔ∏èüîó Langchain', 'description': 'The Fleet AI team is on a mission to embed the world‚Äôs most important', 'language': 'en'}),\n",
       " Document(page_content='# MultiQueryRetriever\\n\\nDistance-based vector database retrieval embeds (represents) queries in\\nhigh-dimensional space and finds similar embedded documents based on\\n‚Äúdistance‚Äù. But, retrieval may produce different results with subtle\\nchanges in query wording or if the embeddings do not capture the\\nsemantics of the data well. Prompt engineering / tuning is sometimes\\ndone to manually address these problems, but can be tedious.\\n\\nThe `MultiQueryRetriever` automates the process of prompt tuning by\\nusing an LLM to generate multiple queries from different perspectives\\nfor a given user input query. For each query, it retrieves a set of\\nrelevant documents and takes the unique union across all queries to get\\na larger set of potentially relevant documents. By generating multiple\\nperspectives on the same question, the `MultiQueryRetriever` might be\\nable to overcome some of the limitations of the distance-based retrieval\\nand get a richer set of results.\\n\\n```python\\n# Build a sample vectorDB\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n# Load blog post\\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\\ndata = loader.load()\\n\\n# Split\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\\nsplits = text_splitter.split_documents(data)\\n\\n# VectorDB\\nembedding = OpenAIEmbeddings()\\nvectordb = Chroma.from_documents(documents=splits, embedding=embedding)\\n```\\n\\n#### Simple usage\\u200b\\n\\nSpecify the LLM to use for query generation, and the retriever will do\\nthe rest.\\n\\n```python\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\nfrom langchain_openai import ChatOpenAI\\n\\nquestion = \"What are the approaches to Task Decomposition?\"\\nllm = ChatOpenAI(temperature=0)\\nretriever_from_llm = MultiQueryRetriever.from_llm(\\n    retriever=vectordb.as_retriever(), llm=llm\\n)\\n```\\n\\n```python\\n# Set logging for the queries\\nimport logging\\n\\nlogging.basicConfig()\\nlogging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\\n```\\n\\n```python\\nunique_docs = retriever_from_llm.get_relevant_documents(query=question)\\nlen(unique_docs)\\n```\\n\\n```text\\nINFO:langchain.retrievers.multi_query:Generated queries: [\\'1. How can Task Decomposition be approached?\\', \\'2. What are the different methods for Task Decomposition?\\', \\'3. What are the various approaches to decomposing tasks?\\']\\n```\\n\\n```text\\n5\\n```\\n\\n#### Supplying your own prompt\\u200b\\n\\nYou can also supply a prompt along with an output parser to split the\\nresults into a list of queries.\\n\\n```python\\nfrom typing import List\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom langchain.prompts import PromptTemplate\\nfrom pydantic import BaseModel, Field\\n\\n# Output parser will split the LLM result into a list of queries\\nclass LineList(BaseModel):\\n    # \"lines\" is the key (attribute name) of the parsed output\\n    lines: List[str] = Field(description=\"Lines of text\")\\n\\nclass LineListOutputParser(PydanticOutputParser):\\n    def __init__(self) -> None:\\n        super().__init__(pydantic_object=LineList)\\n\\n    def parse(self, text: str) -> LineList:\\n        lines = text.strip().split(\"\\\\n\")\\n        return LineList(lines=lines)\\n\\noutput_parser = LineListOutputParser()\\n\\nQUERY_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"You are an AI language model assistant. Your task is to generate five \\n    different versions of the given user question to retrieve relevant documents from a vector \\n    database. By generating multiple perspectives on the user question, your goal is to help\\n    the user overcome some of the limitations of the distance-based similarity search. \\n    Provide these alternative questions separated by newlines.\\n    Original question: {question}\"\"\",\\n)\\nllm = ChatOpenAI(temperature=0)\\n\\n# Chain\\nllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\\n\\n# Other inputs\\nquestion = \"What are the approaches to Task Decomposition?\"\\n```\\n\\n```python\\n# Run\\nretriever = MultiQueryRetriever(\\n    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\\n)  # \"lines\" is the key (attribute name) of the parsed output\\n\\n# Results\\nunique_docs = retriever.get_relevant_documents(\\n    query=\"What does the course say about regression?\"\\n)\\nlen(unique_docs)\\n```\\n\\n```text\\nINFO:langchain.retrievers.multi_query:Generated queries: [\"1. What is the course\\'s perspective on regression?\", \\'2. Can you provide information on regression as discussed in the course?\\', \\'3. How does the course cover the topic of regression?\\', \"4. What are the course\\'s teachings on regression?\", \\'5. In relation to the course, what is mentioned about regression?\\']\\n```\\n\\n```text\\n11\\n```', metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever', 'title': 'MultiQueryRetriever | ü¶úÔ∏èüîó Langchain', 'description': 'Distance-based vector database retrieval embeds (represents) queries in', 'language': 'en'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_documents_retriever = retriever.get_relevant_documents(\n",
    "    query,\n",
    ")\n",
    "full_documents_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes corroborar que el `ParentDocumentRetriever` est√° regresando el subconjunto `√∫nico` de documentos completos al comparar el n√∫mero de documentos recuperados por el `VectorStore` y el `ParentDocumentRetriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://python.langchain.com/docs/integrations/retrievers/fleet_context',\n",
       "  'https://python.langchain.com/docs/integrations/retrievers/fleet_context',\n",
       "  'https://python.langchain.com/docs/integrations/retrievers/fleet_context',\n",
       "  'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever'],\n",
       " ['https://python.langchain.com/docs/integrations/retrievers/fleet_context',\n",
       "  'https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"source\"] for doc in full_documents_similarity], [\n",
    "    doc.metadata[\"source\"] for doc in full_documents_retriever\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperaci√≥n de fragmentos largos en lugar de documentos completos\n",
    "\n",
    "Los documentos pueden ser muy grandes para ser recuperados en su totalidad y ser √∫tiles. \n",
    "\n",
    "Por ejemplo, un documento completo podr√≠a ser un libro, pero quiz√° s√≥lo necesito un cap√≠tulo para responder a mi pregunta. O quiz√° s√≥lo necesito un par de p√°rrafos.\n",
    "\n",
    "Si planeas utilizar los documentos recuperados en un proceso de `Retrival Augmented Generation` (RAG), es posible que los documentos gigantes ni siquiera puedan ser procesados por la ventana de contexto del modelo de lenguaje.\n",
    "\n",
    "Para este caso, el `ParentDocumentRetriever` puede ser configurado para romper los documentos en fragmentos peque√±os, buscar sobre ellos y luego devolver fragmentos m√°s largos (sin ser el documento completo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add parent splitter\n",
    "parent_splitter=RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=40,\n",
    "    length_function=num_tokens_from_string\n",
    ")\n",
    "child_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "\n",
    "vectorstore = get_vectorstore(collection_name=\"big_fragments\")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hay m√°s documentos en el `Store` dado que cada documento se ha dividido en fragmentos m√°s peque√±os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\n",
    "    query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    query,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
