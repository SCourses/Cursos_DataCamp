{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking por relevancia marginal m√°xima (MMR)\n",
    "\n",
    "MMR es un m√©todo de re-ranking que combina la relevancia y la diversidad de los documentos recuperados. El objetivo es maximizar la relevancia de los documentos devueltos y minimizar la redundancia entre ellos.\n",
    "\n",
    "Esto puede ser √∫til para incrementar la habilidad de los modelos de lenguage para generar respuestas con mayor cobertura y profundidad.\n",
    "\n",
    "Su algoritmo es el siguiente:\n",
    "\n",
    "1. Calcular los `embeddings` para cada documento y para la consulta.\n",
    "2. Seleccionar el documento m√°s relevante para la consulta.\n",
    "3. Para cada documento restante, calcular el promedio de similitud de los documentos ya seleccionados.\n",
    "4. Seleccionar el documento que es, en promedio, menos similar a los documentos ya seleccionados.\n",
    "5. Repitir los pasos 3 y 4 hasta que se hayan seleccionado `k` documentos. Es decir, una lista ordenada que parte del documento que m√°s contribuye a la diversidad general hasta el documento que contribuye menos.\n",
    "\n",
    "En Langchain, el algoritmo de MMR es utilizado despu√©s de que el `retriever` ha recuperado los documentos m√°s relevantes para la consulta. Por lo tanto, nos aseguramos que estamos seleccionando documentos diversos de un conjunto de documentos que ya son relevantes para la consulta.\n",
    "\n",
    "![Re-ranking with MMR](./diagrams/slide_diagrama_04_V2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from src.langchain_docs_loader import load_langchain_docs_splitted\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_langchain_docs_splitted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n de retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente creamos nuestro retriever de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_retriever = Chroma.from_documents(\n",
    "    documents=docs[:100],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ").as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, podr√°s notar que de hacerlo, el tipo de b√∫squeda que se realiza es por similitud de vectores. En este caso, queremos realizar una b√∫squeda por similitud de vectores, pero con un re-ranking por relevancia marginal m√°xima (MMR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_retriever.search_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, para crear un retriever con re-ranking por MMR, debemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_retriever = Chroma.from_documents(\n",
    "    documents=docs[:100],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ").as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    k=4,  # number of documents to retrieve after mmr\n",
    "    fetch_k=20,  # number of documents to fetch in the first step\n",
    "    # Lambda mult is a number between 0 and 1 that determines the degree\n",
    "    # of diversity among the results with 0 corresponding to maximum diversity\n",
    "    # and 1 to minimum diversity.\n",
    "    lambda_mult=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nuestro retriever est√° listo para ser utilizado con re-ranking por MMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mmr'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retriever.search_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='## RAG Search Example\\u200b\\n\\nFor our next example, we want to run a retrieval-augmented generation\\nchain to add some context when responding to questions.\\n\\n```python\\n# Requires:\\n# pip install langchain docarray tiktoken\\n\\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_openai.embeddings import OpenAIEmbeddings\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\nIn this case, the composed chain is:\\n\\n```python\\nchain = setup_and_retrieval | prompt | model | output_parser\\n```\\n\\nTo explain this, we first can see that the prompt template above takes\\nin `context` and `question` as values to be substituted in the prompt.\\nBefore building the prompt template, we want to retrieve relevant\\ndocuments to the search and include them as part of the context.\\n\\nAs a preliminary step, we‚Äôve setup the retriever using an in memory\\nstore, which can retrieve documents based on a query. This is a runnable\\ncomponent as well that can be chained together with other components,\\nbut you can also try to run it separately:\\n\\n```python\\nretriever.invoke(\"where did harrison work?\")\\n```\\n\\nWe then use the `RunnableParallel` to prepare the expected inputs into\\nthe prompt by using the entries for the retrieved documents as well as\\nthe original user question, using the retriever for document search, and\\nRunnablePassthrough to pass the user‚Äôs question:\\n\\n```python\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\n```\\n\\nTo review, the complete chain is:\\n\\n```python\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n```\\n\\nWith the flow being:\\n\\n1. The first steps create a `RunnableParallel` object with two entries.\\nThe first entry, `context` will include the document results fetched\\nby the retriever. The second entry, `question` will contain the\\nuser‚Äôs original question. To pass on the question, we use\\n`RunnablePassthrough` to copy this entry.\\n\\n2. Feed the dictionary from the step above to the `prompt` component.\\nIt then takes the user input which is `question` as well as the\\nretrieved document which is `context` to construct a prompt and\\noutput a PromptValue.  \\n\\n3. The `model` component takes the generated prompt, and passes into\\nthe OpenAI LLM model for evaluation. The generated output from the\\nmodel is a `ChatMessage` object.\\n\\n4. Finally, the `output_parser` component takes in a `ChatMessage`, and\\ntransforms this into a Python string, which is returned from the\\ninvoke method.\\n\\n## Next steps\\u200b\\n\\nWe recommend reading our [Why use LCEL](/docs/expression_language/why)\\nsection next to see a side-by-side comparison of the code needed to\\nproduce common functionality with and without LCEL.', metadata={'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='## RAG Search Example\\u200b\\n\\nFor our next example, we want to run a retrieval-augmented generation\\nchain to add some context when responding to questions.\\n\\n```python\\n# Requires:\\n# pip install langchain docarray tiktoken\\n\\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_openai.embeddings import OpenAIEmbeddings\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\nIn this case, the composed chain is:\\n\\n```python\\nchain = setup_and_retrieval | prompt | model | output_parser\\n```\\n\\nTo explain this, we first can see that the prompt template above takes\\nin `context` and `question` as values to be substituted in the prompt.\\nBefore building the prompt template, we want to retrieve relevant\\ndocuments to the search and include them as part of the context.\\n\\nAs a preliminary step, we‚Äôve setup the retriever using an in memory\\nstore, which can retrieve documents based on a query. This is a runnable\\ncomponent as well that can be chained together with other components,\\nbut you can also try to run it separately:\\n\\n```python\\nretriever.invoke(\"where did harrison work?\")\\n```\\n\\nWe then use the `RunnableParallel` to prepare the expected inputs into\\nthe prompt by using the entries for the retrieved documents as well as\\nthe original user question, using the retriever for document search, and\\nRunnablePassthrough to pass the user‚Äôs question:\\n\\n```python\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\n```\\n\\nTo review, the complete chain is:\\n\\n```python\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n```\\n\\nWith the flow being:\\n\\n1. The first steps create a `RunnableParallel` object with two entries.\\nThe first entry, `context` will include the document results fetched\\nby the retriever. The second entry, `question` will contain the\\nuser‚Äôs original question. To pass on the question, we use\\n`RunnablePassthrough` to copy this entry.\\n\\n2. Feed the dictionary from the step above to the `prompt` component.\\nIt then takes the user input which is `question` as well as the\\nretrieved document which is `context` to construct a prompt and\\noutput a PromptValue.  \\n\\n3. The `model` component takes the generated prompt, and passes into\\nthe OpenAI LLM model for evaluation. The generated output from the\\nmodel is a `ChatMessage` object.\\n\\n4. Finally, the `output_parser` component takes in a `ChatMessage`, and\\ntransforms this into a Python string, which is returned from the\\ninvoke method.\\n\\n## Next steps\\u200b\\n\\nWe recommend reading our [Why use LCEL](/docs/expression_language/why)\\nsection next to see a side-by-side comparison of the code needed to\\nproduce common functionality with and without LCEL.', metadata={'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# RAG\\n\\nLet‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds\\nup to a ‚Äúretrieval-augmented generation‚Äù chain\\n\\n```python\\n%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken\\n```\\n\\n```python\\nfrom operator import itemgetter\\n\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n```\\n\\n```python\\nvectorstore = FAISS.from_texts(\\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nmodel = ChatOpenAI()\\n```\\n\\n```python\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\n```text\\n\\'Harrison worked at Kensho.\\'\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in the following language: {language}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = (\\n    {\\n        \"context\": itemgetter(\"question\") | retriever,\\n        \"question\": itemgetter(\"question\"),\\n        \"language\": itemgetter(\"language\"),\\n    }\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\n```\\n\\n```text\\n\\'Harrison ha lavorato a Kensho.\\'\\n```\\n\\n## Conversational Retrieval Chain\\u200b\\n\\nWe can easily add in conversation history. This primarily means adding\\nin chat_message_history\\n\\n```python\\nfrom langchain.schema import format_document\\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\\nfrom langchain_core.runnables import RunnableParallel\\n```\\n\\n```python\\nfrom langchain.prompts.prompt import PromptTemplate\\n\\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\"\"\"\\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nANSWER_PROMPT = ChatPromptTemplate.from_template(template)\\n```\\n\\n```python\\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\\n\\ndef _combine_documents(\\n    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\\\n\\\\n\"\\n):\\n    doc_strings = [format_document(doc, document_prompt) for doc in docs]\\n    return document_separator.join(doc_strings)\\n```\\n\\n```python\\n_inputs = RunnableParallel(\\n    standalone_question=RunnablePassthrough.assign(\\n        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\\n    )\\n    | CONDENSE_QUESTION_PROMPT\\n    | ChatOpenAI(temperature=0)\\n    | StrOutputParser(),\\n)\\n_context = {\\n    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\\n    \"question\": lambda x: x[\"standalone_question\"],\\n}\\nconversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\\n```\\n\\n```python\\nconversational_qa_chain.invoke(\\n    {\\n        \"question\": \"where did harrison work?\",\\n        \"chat_history\": [],\\n    }\\n)\\n```\\n\\n```text\\nAIMessage(content=\\'Harrison was employed at Kensho.\\')\\n```\\n\\n```python\\nconversational_qa_chain.invoke(\\n    {\\n        \"question\": \"where did he work?\",\\n        \"chat_history\": [\\n            HumanMessage(content=\"Who wrote this notebook?\"),\\n            AIMessage(content=\"Harrison\"),\\n        ],\\n    }\\n)\\n```\\n\\n```text\\nAIMessage(content=\\'Harrison worked at Kensho.\\')\\n```', metadata={'description': 'Let‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'title': 'RAG | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# RAG\\n\\nLet‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds\\nup to a ‚Äúretrieval-augmented generation‚Äù chain\\n\\n```python\\n%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken\\n```\\n\\n```python\\nfrom operator import itemgetter\\n\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n```\\n\\n```python\\nvectorstore = FAISS.from_texts(\\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nmodel = ChatOpenAI()\\n```\\n\\n```python\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\n```text\\n\\'Harrison worked at Kensho.\\'\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer in the following language: {language}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = (\\n    {\\n        \"context\": itemgetter(\"question\") | retriever,\\n        \"question\": itemgetter(\"question\"),\\n        \"language\": itemgetter(\"language\"),\\n    }\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\n```\\n\\n```text\\n\\'Harrison ha lavorato a Kensho.\\'\\n```\\n\\n## Conversational Retrieval Chain\\u200b\\n\\nWe can easily add in conversation history. This primarily means adding\\nin chat_message_history\\n\\n```python\\nfrom langchain.schema import format_document\\nfrom langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\\nfrom langchain_core.runnables import RunnableParallel\\n```\\n\\n```python\\nfrom langchain.prompts.prompt import PromptTemplate\\n\\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\"\"\"\\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\\n```\\n\\n```python\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nANSWER_PROMPT = ChatPromptTemplate.from_template(template)\\n```\\n\\n```python\\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\\n\\ndef _combine_documents(\\n    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\\\n\\\\n\"\\n):\\n    doc_strings = [format_document(doc, document_prompt) for doc in docs]\\n    return document_separator.join(doc_strings)\\n```\\n\\n```python\\n_inputs = RunnableParallel(\\n    standalone_question=RunnablePassthrough.assign(\\n        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\\n    )\\n    | CONDENSE_QUESTION_PROMPT\\n    | ChatOpenAI(temperature=0)\\n    | StrOutputParser(),\\n)\\n_context = {\\n    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\\n    \"question\": lambda x: x[\"standalone_question\"],\\n}\\nconversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\\n```\\n\\n```python\\nconversational_qa_chain.invoke(\\n    {\\n        \"question\": \"where did harrison work?\",\\n        \"chat_history\": [],\\n    }\\n)\\n```\\n\\n```text\\nAIMessage(content=\\'Harrison was employed at Kensho.\\')\\n```\\n\\n```python\\nconversational_qa_chain.invoke(\\n    {\\n        \"question\": \"where did he work?\",\\n        \"chat_history\": [\\n            HumanMessage(content=\"Who wrote this notebook?\"),\\n            AIMessage(content=\"Harrison\"),\\n        ],\\n    }\\n)\\n```\\n\\n```text\\nAIMessage(content=\\'Harrison worked at Kensho.\\')\\n```', metadata={'description': 'Let‚Äôs look at adding in a retrieval step to a prompt and LLM, which adds', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/retrieval', 'title': 'RAG | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_retriever.get_relevant_documents(\n",
    "    \"How to integrate LCEL into my Retrieval augmented generation system with a keyword search retriever?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='## RAG Search Example\\u200b\\n\\nFor our next example, we want to run a retrieval-augmented generation\\nchain to add some context when responding to questions.\\n\\n```python\\n# Requires:\\n# pip install langchain docarray tiktoken\\n\\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_openai.embeddings import OpenAIEmbeddings\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\"where did harrison work?\")\\n```\\n\\nIn this case, the composed chain is:\\n\\n```python\\nchain = setup_and_retrieval | prompt | model | output_parser\\n```\\n\\nTo explain this, we first can see that the prompt template above takes\\nin `context` and `question` as values to be substituted in the prompt.\\nBefore building the prompt template, we want to retrieve relevant\\ndocuments to the search and include them as part of the context.\\n\\nAs a preliminary step, we‚Äôve setup the retriever using an in memory\\nstore, which can retrieve documents based on a query. This is a runnable\\ncomponent as well that can be chained together with other components,\\nbut you can also try to run it separately:\\n\\n```python\\nretriever.invoke(\"where did harrison work?\")\\n```\\n\\nWe then use the `RunnableParallel` to prepare the expected inputs into\\nthe prompt by using the entries for the retrieved documents as well as\\nthe original user question, using the retriever for document search, and\\nRunnablePassthrough to pass the user‚Äôs question:\\n\\n```python\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\n```\\n\\nTo review, the complete chain is:\\n\\n```python\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n```\\n\\nWith the flow being:\\n\\n1. The first steps create a `RunnableParallel` object with two entries.\\nThe first entry, `context` will include the document results fetched\\nby the retriever. The second entry, `question` will contain the\\nuser‚Äôs original question. To pass on the question, we use\\n`RunnablePassthrough` to copy this entry.\\n\\n2. Feed the dictionary from the step above to the `prompt` component.\\nIt then takes the user input which is `question` as well as the\\nretrieved document which is `context` to construct a prompt and\\noutput a PromptValue.  \\n\\n3. The `model` component takes the generated prompt, and passes into\\nthe OpenAI LLM model for evaluation. The generated output from the\\nmodel is a `ChatMessage` object.\\n\\n4. Finally, the `output_parser` component takes in a `ChatMessage`, and\\ntransforms this into a Python string, which is returned from the\\ninvoke method.\\n\\n## Next steps\\u200b\\n\\nWe recommend reading our [Why use LCEL](/docs/expression_language/why)\\nsection next to see a side-by-side comparison of the code needed to\\nproduce common functionality with and without LCEL.', metadata={'description': 'LCEL makes it easy to build complex chains from basic components, and', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/get_started', 'title': 'Get started | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# Code writing\\n\\nExample of how to use LCEL to write Python code.\\n\\n```python\\n%pip install --upgrade --quiet  langchain-core langchain-experimental langchain-openai\\n```\\n\\n```python\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import (\\n    ChatPromptTemplate,\\n)\\nfrom langchain_experimental.utilities import PythonREPL\\nfrom langchain_openai import ChatOpenAI\\n```\\n\\n```python\\ntemplate = \"\"\"Write some python code to solve the user\\'s problem. \\n\\nReturn only python code in Markdown format, e.g.:\\n\\n```python\\n....\\n```\"\"\"\\nprompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\\n\\nmodel = ChatOpenAI()\\n```\\n\\n```python\\ndef _sanitize_output(text: str):\\n    _, after = text.split(\"```python\")\\n    return after.split(\"```\")[0]\\n```\\n\\n```python\\nchain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run\\n```\\n\\n```python\\nchain.invoke({\"input\": \"whats 2 plus 2\"})\\n```\\n\\n```text\\nPython REPL can execute arbitrary code. Use with caution.\\n```\\n\\n```text\\n\\'4\\\\n\\'\\n```', metadata={'description': 'Example of how to use LCEL to write Python code.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/cookbook/code_writing', 'title': 'Code writing | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='- [HOW to Make Conversational Form with LangChain](https://youtu.be/IT93On2LB5k)\\n- ‚õì [Claude-2 meets LangChain!](https://youtu.be/Hb_D3p0bK2U?si=j96Kc7oJoeRI5-iC)\\n- ‚õì [PaLM 2 Meets LangChain](https://youtu.be/orPwLibLqm4?si=KgJjpEbAD9YBPqT4)\\n- ‚õì [LLaMA2 with LangChain - Basics | LangChain TUTORIAL](https://youtu.be/cIRzwSXB4Rc?si=v3Hwxk1m3fksBIHN)\\n- ‚õì [Serving LLaMA2 with Replicate](https://youtu.be/JIF4nNi26DE?si=dSazFyC4UQmaR-rJ)\\n- ‚õì [NEW LangChain Expression Language](https://youtu.be/ud7HJ2p3gp0?si=8pJ9O6hGbXrCX5G9)\\n- ‚õì [Building a RCI Chain for Agents with LangChain Expression Language](https://youtu.be/QaKM5s0TnsY?si=0miEj-o17AHcGfLG)\\n- ‚õì [How to Run LLaMA-2-70B on the Together AI](https://youtu.be/Tc2DHfzHeYE?si=Xku3S9dlBxWQukpe)\\n- ‚õì [RetrievalQA with LLaMA 2 70b & Chroma DB](https://youtu.be/93yueQQnqpM?si=ZMwj-eS_CGLnNMXZ)\\n- ‚õì [How to use BGE Embeddings for LangChain](https://youtu.be/sWRvSG7vL4g?si=85jnvnmTCF9YIWXI)\\n- ‚õì [How to use Custom Prompts for RetrievalQA on LLaMA-2 7B](https://youtu.be/PDwUKves9GY?si=sMF99TWU0p4eiK80)', metadata={'description': 'Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.', 'language': 'en', 'source': 'https://python.langchain.com/docs/additional_resources/tutorials', 'title': 'Tutorials | ü¶úÔ∏èüîó Langchain'}),\n",
       " Document(page_content='# LangChain Expression Language (LCEL)\\n\\nLangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to **support putting prototypes in production, with no code changes**, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\\n\\n**Streaming support**\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\\n\\n**Async support**\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a [LangServe](/docs/langsmith) server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\\n\\n**Optimized parallel execution**\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\\n\\n**Retries and fallbacks**\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\\n\\n**Access intermediate results**\\nFor more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every [LangServe](/docs/langserve) server.\\n\\n**Input and output schemas**\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\\n\\n**Seamless LangSmith tracing integration**\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](/docs/langsmith/) for maximum observability and debuggability.\\n\\n**Seamless LangServe deployment integration**\\nAny chain created with LCEL can be easily deployed using [LangServe](/docs/langserve).', metadata={'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en', 'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó Langchain'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retriever.get_relevant_documents(\n",
    "    \"How to integrate LCEL into my Retrieval augmented generation system with a keyword search retriever?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
