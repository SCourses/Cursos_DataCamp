{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "os.environ['OPENAI_API_KEY']='eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJhcHAiLCJzdWIiOiIxNjMxMDQ1IiwiYXVkIjoiV0VCIiwiaWF0IjoxNzA2MDU2ODg0LCJleHAiOjE3MDg2NDg4ODR9.y_RfgCyCANoLM9_HquGHoteMOinB6H694ggbAvbm7kY'\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=26\n",
    "chunk_overlap=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "c_splitter=CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why doesn't this split the string below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1='abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2='abcdefghijklmnopqrstuvwxyzabcdefghijklmoprst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefghijklmoprst']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3=\"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CharacterTextSplitter split text to a predefinied variable, this variable is newline, and in our text don't have newlines, the text isn´t split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m n o p q r s t u v w x y z']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter=CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive splitting details\n",
    "`RecursiveCharacterTextSplitter` is recommended for generic text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter=RecursiveCharacterTextSplitter(chunk_size=450,chunk_overlap=0,separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "c_splitter=CharacterTextSplitter(chunk_size=450,chunk_overlap=0,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter=RecursiveCharacterTextSplitter(chunk_size=150,chunk_overlap=0,separators=[\"\\n\\n\",\"\\.\",\"\\n\",\" \",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter=RecursiveCharacterTextSplitter(chunk_size=150,chunk_overlap=0,separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"])\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/BROS.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BROS: A Pre-trained Language Model Focusing on Text and Layout\\nfor Better Key Information Extraction from Documents\\nTeakgyu Hong1, Donghyun Kim1, Mingi Ji2, Wonseok Hwang3, Daehyun Nam4, Sungrae Park4\\n1NA VER CLOV A,2KAIST,3LBox,4Upstage AI Research, Upstage AI\\nteakgyu.hong@navercorp.com, dong.hyun@navercorp.com, qwertgfdcvb@kaist.ac.kr,\\nwonseok.hwang@lbox.kr, daehyun.nam@upstage.ai, sungrae.park@upstage.ai\\nAbstract\\nKey information extraction (KIE) from document images re-\\nquires understanding the contextual and spatial semantics of\\ntexts in two-dimensional (2D) space. Many recent studies try\\nto solve the task by developing pre-trained language mod-\\nels focusing on combining visual features from document\\nimages with texts and their layout. On the other hand, this\\npaper tackles the problem by going back to the basic: ef-\\nfective combination of text and layout. Speciﬁcally, we pro-\\npose a pre-trained language model, named BROS (BERT Re-', metadata={'source': 'data/BROS.pdf', 'page': 0}),\n",
       " Document(page_content='fective combination of text and layout. Speciﬁcally, we pro-\\npose a pre-trained language model, named BROS (BERT Re-\\nlying On Spatiality) , that encodes relative positions of texts\\nin 2D space and learns from unlabeled documents with area-\\nmasking strategy. With this optimized training scheme for un-\\nderstanding texts in 2D space, BROS shows comparable or\\nbetter performance compared to previous methods on four\\nKIE benchmarks (FUNSD, SROIE∗, CORD, and SciTSR)\\nwithout relying on visual features. This paper also reveals\\ntwo real-world challenges in KIE tasks–(1) minimizing the\\nerror from incorrect text ordering and (2) efﬁcient learning\\nfrom fewer downstream examples–and demonstrates the su-\\nperiority of BROS over previous methods. Code is available\\nathttps://github.com/clovaai/bros .\\nIntroduction\\nAutomatic key information extraction (KIE) from industrial\\ndocuments is an essential task in robotic process automation\\n(RPA). Extracting an ordered item list from receipts (Park', metadata={'source': 'data/BROS.pdf', 'page': 0}),\n",
       " Document(page_content='documents is an essential task in robotic process automation\\n(RPA). Extracting an ordered item list from receipts (Park\\net al. 2019), prices and taxes from invoices (Liu et al. 2019),\\nand paired key-values from form-like documents (Jaume,\\nEkenel, and Thiran 2019) are representative examples. Since\\nthe task requires understanding texts in various layouts, the\\ncombination of multiple technical components from both\\ncomputer vision and natural language processing is required.\\nFigure 1 describes a schematic illustration of pipeline for\\nthe document KIE tasks (Hwang et al. 2019; Denk and Reis-\\nswig 2019). First, given a document image, optical character\\nrecognition (OCR) detects the texts in the image and recog-\\nnizes the content to generate a set of text blocks. Next, a\\nserializer identiﬁes a reading order of text blocks distributed\\nin 2D image space and converts them into text sequence in\\n1D text space to apply NLP technology which is developed', metadata={'source': 'data/BROS.pdf', 'page': 0}),\n",
       " Document(page_content='in 2D image space and converts them into text sequence in\\n1D text space to apply NLP technology which is developed\\nfor 1D text sequence. The most basic form of serializer is\\nto arrange text blocks in a top-to-bottom and left-to-right\\nway (Clausner, Pletschacher, and Antonacopoulos 2013). Fi-\\nOCRParsing\\nModelDate\\n9/3/92\\nR&D Group\\nLicensee...Serializer\\nFigure 1: Schematic illustrations of document KIE pipeline.\\nModel Img # Params (O) (P) (F)\\nLayoutLM BASE 113M 78.66 33.89 62.50\\nLayoutLMv2 BASE◦ 200M 82.76 40.77 69.92\\nBROS BASE 110M 83.05 76.94 72.60\\nLayoutLM LARGE 343M 78.95 33.11 61.00\\nLayoutLMv2 LARGE◦ 426M 84.20 62.53 72.12\\nBROS LARGE 340M 84.52 79.42 74.42\\nTable 1: Performance comparison of pre-trained language\\nmodels on (O)riginal, (P)ermuted, and (F)ew training sam-\\nples FUNSD KIE tasks. In (F), 10 samples are used.\\nnally, from the serialized text blocks, key information is ex-\\ntracted via the parsing model.\\nIn the ﬁrst step, an off-the-shelf OCR tool is often em-', metadata={'source': 'data/BROS.pdf', 'page': 0}),\n",
       " Document(page_content='nally, from the serialized text blocks, key information is ex-\\ntracted via the parsing model.\\nIn the ﬁrst step, an off-the-shelf OCR tool is often em-\\nployed as industrial documents consist of relatively clean\\ncharacters compared to general scene text images. On the\\nother hand, there are no such off-the-shelf tools for the se-\\nrializer even though it is often non-trivial to determine the\\nproper reading order of text blocks (Li et al. 2020; Wang\\net al. 2021). Representative examples are documents includ-\\ning multi-columns or multiple tables. This absence of the\\ngeneral-purpose serializer implies the careful design of pars-\\ning module is necessary to robustly handle documents with\\ncomplex layouts where the reading orders can be often in-\\ncomplete.\\nIn the early studies of KIE, accurate document parsing\\ngreatly depends on the order of text blocks. Once the se-\\nrializer identiﬁes an order, the set of the text blocks are\\nconverted into a text sequence and processed via a lan-', metadata={'source': 'data/BROS.pdf', 'page': 0}),\n",
       " Document(page_content='rializer identiﬁes an order, the set of the text blocks are\\nconverted into a text sequence and processed via a lan-\\nguage model such as BERT (Devlin et al. 2019) to identify\\nkey information (Hwang et al. 2019; Denk and Reisswig\\n2019). The linguistic understanding of the pre-trained lan-\\nguage model leads to superior performance than rule-based\\nextractions. However, the conversion of texts in 2D space\\ninto a text sequence in 1D space leads to the loss of layoutarXiv:2108.04539v5  [cs.CL]  5 Apr 2022', metadata={'source': 'data/BROS.pdf', 'page': 0}),\n",
       " Document(page_content='information that is critical in KIE tasks.\\nTo avoid the loss of layout information, a new type of\\nlanguage model, LayoutLM (Xu et al. 2020) expands a 1D\\npositional encoding of BERT to 2D and is trained over a\\nlarge corpus of industrial documents to understand spatial\\ndependencies between text blocks. Its ﬁne-tuning has shown\\nbreakthrough performances on multiple KIE tasks and be-\\ncomes a strong baseline. After the rise of LayoutLM, several\\nstudies try to develop pre-trained language models by com-\\nbining additional visual features (Xu et al. 2021; Powalski\\net al. 2021; Li et al. 2021b; Appalaraju et al. 2021; Li et al.\\n2021c) (e.g. image patches identiﬁed by an object detection)\\nand show further performance improvements. However, the\\nextensions using visual features require additional computa-\\ntional costs and they still demand more effective combina-\\ntions of texts and their spatial information.\\nIn this paper, we introduce a new pre-trained language', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='tions of texts and their spatial information.\\nIn this paper, we introduce a new pre-trained language\\nmodel, named BROS, by re-focusing on the combinations\\nof texts and their spatial information without relying on\\nvisual features. Speciﬁcally, we propose an effective spa-\\ntial encoding method by utilizing relative positions between\\ntext blocks, while most of previous works employ abso-\\nlute 2D positions. Additionally, we introduce a novel self-\\nsupervision method, named area-masked language model,\\nthat hides texts in an area of a document and supervises\\nthe masked texts. With these two approaches for encoding\\nof spatial information, BROS shows superior or compara-\\nble performances compared to previous methods using ad-\\nditional visual features.\\nAside from improving KIE performances, BROS also ad-\\ndresses two important real-world challenges in KIE tasks:\\nminimizing dependency on the order of text blocks and\\nlearning from a few training examples of downstream tasks.', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='minimizing dependency on the order of text blocks and\\nlearning from a few training examples of downstream tasks.\\nThe ﬁrst challenge indicates the robustness on the serializa-\\ntion followed by the OCR process in Figure 1. In real sce-\\nnario, document images are usually irregular (i.e. rotated or\\ndistorted documents) and the serializer might fail to iden-\\ntify a proper order of text blocks. In addition, when se-\\nrialization fails, the performance of sequence tagging ap-\\nproaches (e.g. BIO tagging), which most previous works\\nemploy, drops dramatically. To circumvent the difﬁculty, we\\napply SPADE (Hwang et al. 2021) decoder that extracts key\\ntext blocks without any order information to the pre-trained\\nmodels and evaluates them on the new benchmarks where\\nthe order of text blocks are permuted. As a result, BROS\\nshows better robustness on the serializers compared to Lay-\\noutLM (Xu et al. 2020) and LayoutLMv2 (Xu et al. 2021).\\nThe second challenge is related to the required number', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='outLM (Xu et al. 2020) and LayoutLMv2 (Xu et al. 2021).\\nThe second challenge is related to the required number\\nof labeled examples to understand the target key contents.\\nSince a single KIE example consists of hundreds of text\\nblocks that should be categorized, the annotation is expen-\\nsive. Most public benchmarks consist of less than 1,000\\nsamples, even though the target documents contain hundreds\\nof layouts and diverse contexts. In this paper, we analyze\\nKIE performances over the number of training examples\\nand compare the pre-trained models. As a result, BROS per-\\nforms better on FUNSD KIE tasks, and also BROS only with\\n20∼30% of FUNSD examples achieves better performance\\nthan LayoutLM with 100% of them. Summarized results forthese experiments are shown in Table 1.\\nOur contributions can be summarized as follows:\\n• We propose an effective spatial layout encoding method\\nby accounting for relative positions of text blocks.\\n• We also propose a novel area-masking self-supervision', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='by accounting for relative positions of text blocks.\\n• We also propose a novel area-masking self-supervision\\nstrategy that reﬂects 2D natures of text blocks.\\n• The proposed model achieves comparable performance\\nto the state-of-the-art without relying on visual features.\\n• We compare existing pre-trained models on permuted\\nKIE datasets that lost the orders of text blocks.\\n• We compare the ﬁne-tuning efﬁciency of various pre-\\ntrained models under a data-scarce environment.\\nRelated Work\\nPre-trained Language Models for 2D Text Blocks\\nUnlike the pre-trained models for conventional NLP tasks,\\nsuch as BERT (Devlin et al. 2019), LayoutLM (Xu et al.\\n2020) is ﬁrst proposed to jointly model interaction between\\ntext and layout information for the document KIE task. It\\nencodes the absolute position of text blocks with axis-wise\\nembedding tables and learns a token-level masked language\\nmodel that hides tokens randomly and estimates the origins.', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='embedding tables and learns a token-level masked language\\nmodel that hides tokens randomly and estimates the origins.\\nAfter the publication of LayoutLM, several pre-trained mod-\\nels have been tried to additionally integrate visual features,\\nsuch as visual feature maps from raw images (Xu et al. 2021;\\nAppalaraju et al. 2021), image patches identiﬁed by an ob-\\nject detection module (Li et al. 2021b), and visual represen-\\ntations of text blocks (Powalski et al. 2021; Li et al. 2021c).\\nAlthough the extensions imposing multi-modalities of visual\\nand textual features provide additional performance gains in\\nKIE tasks, they spend additional computations to process\\nraw document images. Additionally, an effective combina-\\ntion of text and layout is still required as the major compo-\\nnent of the multi-modalities.\\nAside from incorporating visual features, Struc-\\nturalLM (Li et al. 2021a) utilizes cell information, a\\ngroup of ordered text blocks, and shows promising per-', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='turalLM (Li et al. 2021a) utilizes cell information, a\\ngroup of ordered text blocks, and shows promising per-\\nformance improvements. However, the local orders of text\\nblocks might not be available depending on the KIE tasks\\nand the OCR engines. Therefore, this paper focuses on\\nthe original granularity of text blocks identiﬁed by OCR\\nengines and improves the combination of text and layout\\nby an effective spatial encoding method and an area-based\\npre-training strategy.\\nParsers for Document Key Information Extraction\\nBIO tagger, which is a representative parser for entity ex-\\ntraction from the text sequences, extracts key information\\nby identifying spans with the beginning (B) and inside (I)\\npoints. Though BIO tagger has been used as a conventional\\nmethod, it has two limitations for applying to document KIE.\\nOne is that the correct order of text blocks is required for\\nextracting key information when post-processing each clas-\\nsiﬁed token class (i.e. B- and I- classes). For example, if the', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='extracting key information when post-processing each clas-\\nsiﬁed token class (i.e. B- and I- classes). For example, if the\\ntext blocks are not ordered properly, such as “recognition,\\noptical, character”, the correct answer can not be made. The', metadata={'source': 'data/BROS.pdf', 'page': 1}),\n",
       " Document(page_content='TokensBROS Encoder\\nPositions\\nemgeddings...\\nTokenemgeddings ...\\n[CLS] if... reports [MASK] the [MASK] [MASK] [SEP]+++++ ++++...Pre-training / Fine-tuning\\nPositions(+2,0) (+6,0) (-8,0)(+1,-2)Relative spatial encoding\\nToken- & area-\\nmasking\\nFigure 2: An overview of BROS. The tokens in the document image are masked through token- and area-masking strategy.\\nThe position difference between text blocks is encoded directly to the attention mechanism in Transformer. The output token\\nrepresentations are used in both pre-training and ﬁne-tuning.\\nother is that it cannot solve the tasks that require the relation-\\nship between tokens as it performs token-level classiﬁcation.\\nTo overcome the above two limitations, we adopt a graph-\\nbased parser, SPADE (Hwang et al. 2021) decoder, that cre-\\nates a directed relation graph of tokens to represent key\\nentities and their relationships for KIE tasks. For exam-\\nple, SPADE can determine “optical” as a starting word and', metadata={'source': 'data/BROS.pdf', 'page': 2}),\n",
       " Document(page_content='entities and their relationships for KIE tasks. For exam-\\nple, SPADE can determine “optical” as a starting word and\\n“recognition” as the next word. By directly identifying re-\\nlations between tokens, SPADE enables a description of all\\nkey information of KIE tasks regardless of the order of text\\nblocks. In this paper, we apply the SPADE decoder for en-\\ntity linking tasks of KIE benchmarks and also for all tasks\\nlost perfect order information of text blocks. Speciﬁcally, we\\nslightly modify the SPADE decoder for better application\\nwith the pre-trained models. The details can be found in the\\nAppendix.\\nBERT Relying on Spatiality (BROS)\\nThe main structure of BROS follows LayoutLM (Xu et al.\\n2020), but there are two critical advances: (1) a use of spa-\\ntial encoding metric that describes spatial relations between\\ntext blocks and (2) a use of 2D pre-training objective de-\\nsigned for text blocks on 2D space. Figure 2 shows a visual\\ndescription of BROS for document KIE tasks.', metadata={'source': 'data/BROS.pdf', 'page': 2}),\n",
       " Document(page_content='signed for text blocks on 2D space. Figure 2 shows a visual\\ndescription of BROS for document KIE tasks.\\nEncoding Spatial Information into BERT\\nThe way to encode spatial information of text blocks de-\\ncides how text blocks be aware of their spatial relations.\\nLayoutLM (Xu et al. 2020) simply encodes absolute x- and\\ny-axis positions to each text blocks but the speciﬁc-point en-\\ncoding is not robust on the minor position changes of text\\nblocks. Instead, BROS employs relative positions between\\ntext blocks to explicitly encode spatial relations. As shown\\nin Figure 3, relative positions provides co-modality of spa-\\ntial relations between text blocks regardless of their absolute\\nposition. This property can make the model better recognize\\nentities which have similar key-value structures.\\nFor formal description, we use p= (x,y)to denote a\\n(a) Encodes absolute spatial information.\\n(b) Encodes relative spatial information.\\nFigure 3: Comparison between absolute and relative posi-', metadata={'source': 'data/BROS.pdf', 'page': 2}),\n",
       " Document(page_content='(a) Encodes absolute spatial information.\\n(b) Encodes relative spatial information.\\nFigure 3: Comparison between absolute and relative posi-\\ntions. “Project #” and “Total Sample” have their paired val-\\nues, “72-31” and “285”, respectively. In (a), the paired text\\nblocks have different modalities based their absolute posi-\\ntions. On the other hand, in (b), they can hold co-modality to\\nrepresent positions of their semantically coupled text blocks.\\npoint on 2D space and a bounding box of a text block con-\\nsists of four vertices, such as ptl,ptr,pbr, andpbl, that indi-\\ncate top-left, top-right, bottom-right, and bottom-left points,\\nrespectively. BROS ﬁrst normalizes all the 2D points of the\\ntext blocks using the size of the image. Then, BROS calcu-\\nlates relative positions of the vertices from the same vertices\\nof the other bounding boxes of text blocks and applies si-\\nnusoidal functions as ¯pi,j= [fsinu(xi−xj);fsinu(yi−yj)].\\nHere, fsinu:R→RDsindicates a sinusoidal function, which', metadata={'source': 'data/BROS.pdf', 'page': 2}),\n",
       " Document(page_content='nusoidal functions as ¯pi,j= [fsinu(xi−xj);fsinu(yi−yj)].\\nHere, fsinu:R→RDsindicates a sinusoidal function, which\\nis used in Vaswani et al. (2017), Dsis the dimensions of si-\\nnusoid embedding, and the semicolon (;) indicates concate-\\nnation. Through the calculations, the relative positions of jth\\nbounding box based on the ithbounding box are represented\\nwith the four vectors, such as ¯ptl\\ni,j,¯ptr\\ni,j,¯pbr\\ni,j, and ¯pbl\\ni,j. Fi-\\nnally, BROS combines the four relative positions by apply-\\ning a linear transformation,\\nbbi,j=Wtl¯ptl\\ni,j+Wtr¯ptr\\ni,j+Wbr¯pbr\\ni,j+Wbl¯pbl\\ni,j.(1)\\nwhere Wtl,Wtr,Wbr,Wbl∈R(H/A )×2Dsare linear tran-\\nsition matrices, His a hidden size of BERT, and Ais the', metadata={'source': 'data/BROS.pdf', 'page': 2}),\n",
       " Document(page_content='(a) Random token selection (red) and token masking (gray)\\n(b) Random area selection (red) and block masking (gray)\\nFigure 4: Illustrations of two masking strategies. The blue\\nboxes represent text blocks including masked tokens. In both\\nﬁgures, 15% of tokens are masked.\\nnumber of self-attention heads.\\nIn the process of identifying the relative positional vec-\\ntor,bbi,j, we carefully apply two components: the sinusoidal\\nfunction, fsinu, and the shared embeddings to multiple heads\\nof the attention module. First, the sinusoidal function can en-\\ncode continuous distances more naturally than using a grid\\nembedding that split a real-valued space into ﬁnite num-\\nber of grids. Second, the multi-head attention modules in\\nTransformer share the same relative positional embeddings\\nto impose the common spatial relationships between text\\nblocks to multiple semantic features identiﬁed by the mul-\\ntiple heads.\\nBROS directly encodes the spatial relations to the contex-', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='blocks to multiple semantic features identiﬁed by the mul-\\ntiple heads.\\nBROS directly encodes the spatial relations to the contex-\\ntualization of text blocks. In detail, it calculates an attention\\nlogit combining both semantic and spatial features as fol-\\nlows;\\nah\\ni,j= (Wq\\nhti)⊤(Wk\\nhtj) + (Wq\\nhti)⊤bbi,j, (2)\\nwhere tiandtjare context representations for ithandjth\\ntokens and both Wq\\nhandWk\\nhare linear transition matrices\\nforhthhead. The former is the same as the original attention\\nmechanism in Transformer (Vaswani et al. 2017). The latter,\\nmotivated by Dai et al. (2019), considers the relative spatial\\ninformation of the target text block when the source context\\nand location are given. As we mentioned above, we have\\nshared relative spatial embedding across all of the different\\nattention heads for imposing the common spatial relation-\\nships.\\nCompared to the spatial-aware attention in Xu et al.\\n(2021), which utilizes axis-speciﬁc positional difference of', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='ships.\\nCompared to the spatial-aware attention in Xu et al.\\n(2021), which utilizes axis-speciﬁc positional difference of\\ntext blocks as an attention bias, it has two major differences.\\nFirst, our method couples the relative embeddings with the\\nsemantic information of tokens for better conjugation be-\\ntween texts and their spatial relations. Second, when cal-\\nculating the relative spatial information between two text\\nblocks, we consider all four vertices of the block. By doing\\nthis, our encoding can incorporate not only relative distance\\nbut also relative shape and size which play important roles\\nin distinguishing key and value in a document. We compareour relative encoding method and that of LayoutLMv2’s in\\nthe ablation study.\\nArea-masked Language Model\\nPre-training diverse layouts from unlabeled documents is\\na key factor for document KIE tasks. BROS utilizes two\\npre-training objectives: one is a token-masked LM (TMLM)\\nused in BERT and the other is a novel area-masked LM', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='pre-training objectives: one is a token-masked LM (TMLM)\\nused in BERT and the other is a novel area-masked LM\\n(AMLM) introduced in this paper. The area-masked LM, in-\\nspired by SpanBERT (Joshi et al. 2020), captures consecu-\\ntive text blocks based on a 2D area in a document.\\nTMLM randomly masks tokens while keeping their spa-\\ntial information, and then the model predicts the masked\\ntokens with the clues of spatial information and the other\\nun-masked tokens. The process is identical to MLM of\\nBERT and Masked Visual-Language Model (MVLM) of\\nLayoutLM. Figure 4 (a) shows how TMLM masks tokens in\\na document. Since tokens in a text block can be masked par-\\ntially, their estimation can be conducted by referring to other\\ntokens in the same block or text blocks near the masked to-\\nken.\\nAMLM masks all text blocks allocated in a randomly\\nchosen area. It can be interpreted as a span masking for\\ntext blocks in 2D space. Speciﬁcally, AMLM consists of', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='chosen area. It can be interpreted as a span masking for\\ntext blocks in 2D space. Speciﬁcally, AMLM consists of\\nthe following four steps: (1) randomly selects a text block,\\n(2) identiﬁes an area by expanding the region of the text\\nblock, (3) determines text blocks allocated in the area, and\\n(4) masks all tokens of the text blocks and predicts them.\\nAt the second step, the degree of expansion is identiﬁed\\nby sampling a value from an exponential distribution with\\na hyper-parameter, λ. The rationale behind using exponen-\\ntial distribution is to convert the geometric distribution used\\nin SpanBERT for a discrete domain into a distribution for\\na continuous domain. Thus, we set λ=−ln(1−p)where\\np= 0.2used in SpanBERT. Also, we truncated exponential\\ndistribution with 1 to prevent an inﬁnity value covering all\\nspaces of the document. It should be noted that the masking\\narea is expanded from a randomly selected text block since\\nthe area should be related to the text sizes and locations to', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='area is expanded from a randomly selected text block since\\nthe area should be related to the text sizes and locations to\\nrepresent text spans in 2D space. Figure 4 compares token-\\nand area-masking on text blocks. Because AMLM hides spa-\\ntially close tokens together, their estimation requires more\\nclues from text blocks far from the estimation targets.\\nFinally, BROS combines two masked LMs, TMLM and\\nAMLM, to stimulate the model to learn both individual and\\nconsolidated token representations. It ﬁrst masks 15% of to-\\nkens for AMLM and then masks 15% of tokens on the left\\ntext blocks for TMLM. Similar to BERT (Devlin et al. 2019),\\nthe masked tokens are replaced by [MASK] token for 80%,\\nrandom token for 10%, and original token for the rest 10%.\\nKey Information Extraction Tasks\\nWe solve two categories of KIE tasks, entity extraction\\n(EE) and entity linking (EL). The EE task identiﬁes se-\\nquences of text blocks that represent desired target texts.', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='(EE) and entity linking (EL). The EE task identiﬁes se-\\nquences of text blocks that represent desired target texts.\\nFigure 5 (a) is an example of the EE task: identifying\\nheader, question, and answer entities in the form-like doc-\\nument. The EL task connects key entities through their hi-\\nerarchical or semantic relations. Figure 5 (b) is an example', metadata={'source': 'data/BROS.pdf', 'page': 3}),\n",
       " Document(page_content='(a) An example of FUNSD EE task.\\n(b) An example of CORD EL task.\\nFigure 5: Examples of EE and EL tasks. In (a), the colored\\nblocks represent key entities. In (b), the red arrows show the\\nhierarchical relationships between the entities.\\nDataset Types Tasks # Images\\nFUNSD Forms EE, EL Train 149, Test 50\\nSROIE∗†Receipts EE Train 526, Test 100\\nCORD Receipts EE, EL Train 800, Val 100, Test 100\\nSciTSR Tables EL Train 12,000, Test 3,000\\n†modiﬁed version of SROIE. See details in Appendix.\\nTable 2: Tasks and the number of images for each dataset.\\nof the EL task: grouping menu entities, such as its name,\\nunit price, amount, and price. Table 2 lists four KIE bench-\\nmark datasets: FUNSD (Jaume, Ekenel, and Thiran 2019),\\nSROIE∗(Huang et al. 2019), CORD (Park et al. 2019), and\\nSciTSR (Chi et al. 2019). Details of the benchmarks can be\\nfound in Appendix.\\nAlthough these four datasets provide testbeds for the EE\\nand EL tasks, they represent the subset of real problems as', metadata={'source': 'data/BROS.pdf', 'page': 4}),\n",
       " Document(page_content='found in Appendix.\\nAlthough these four datasets provide testbeds for the EE\\nand EL tasks, they represent the subset of real problems as\\nthe order information of text blocks is given. FUNSD pro-\\nvides the orders of text blocks related to target classes in\\nboth training and testing examples. In SROIE∗, CORD, and\\nSciTSR, the text blocks are serialized in reading orders. To\\nreﬂect the real scenario that does not contain perfect order\\ninformation of text blocks, we remove the order information\\nof KIE benchmarks by randomly permuting the order of text\\nblocks. We denote the permuted datasets as p-FUNSD, p-\\nSROIE∗, p-CORD, and p-SciTSR.\\nExperiments\\nExperiment Settings\\nFor pre-training, IIT-CDIP Test Collection 1.01(Lewis\\net al. 2006), which consists of approximately 11M doc-\\nument images, is used but 400K of RVL-CDIP dataset2\\n(Harley, Ufkes, and Derpanis 2015) are excluded following\\nLayoutLM. To obtain text blocks from document images,\\n1https://ir.nist.gov/cdip/', metadata={'source': 'data/BROS.pdf', 'page': 4}),\n",
       " Document(page_content='(Harley, Ufkes, and Derpanis 2015) are excluded following\\nLayoutLM. To obtain text blocks from document images,\\n1https://ir.nist.gov/cdip/\\n2https://www.cs.cmu.edu/ aharley/rvl-cdip/CLOV A OCR API3was applied. We observed no difference\\nin performance depending on the OCR engine; LayoutLM\\ntrained in our experimental setting shows comparable per-\\nformances to the published LayoutLM. The sanity check can\\nbe found in Appendix.\\nThe main Transformer structure of BROS is the same as\\nBERT. We set the hidden size, the number of self-attention\\nheads, the feed-forward/ﬁlter size, and the number of Trans-\\nformer layers of BROS BASE to 768, 12, 3072, and 12, respec-\\ntively and those of BROS LARGE to 1024, 24, 4096, and 24,\\nrespectively. The dimensions of sinusoid embedding Dsis\\nset to 24 for BROS BASE and 32 for BROS LARGE .\\nBROS is trained by using AdamW optimizer (Loshchilov\\nand Hutter 2019) with a learning rate of 5e-5 with linear\\ndecay. The batch size is set to 64. During pre-training, the', metadata={'source': 'data/BROS.pdf', 'page': 4}),\n",
       " Document(page_content='and Hutter 2019) with a learning rate of 5e-5 with linear\\ndecay. The batch size is set to 64. During pre-training, the\\nﬁrst 10% of the total epochs are used for a warm-up learning\\nrate. We initialized weights of BROS with those of BERT\\nand trained it for 5 epochs on the IIT-CDIP dataset using 8\\nNVIDIA Tesla V100 32GB GPUs.\\nDuring ﬁne-tuning, the learning rate is set to 5e-5. The\\nbatch size is set to 16 for all tasks. The number of training\\nepochs or steps is as follows: 100 epochs for FUNSD, 1K\\nsteps for SROIE∗and CORD, and 7.5 epochs for SciTSR.\\nExperiment Results\\nTo evaluate the performance of the model, we ﬁrst con-\\nduct experiments using the given order of text blocks in the\\ndataset. Then, we verify the robustness of the model against\\ntwo important challenges in the KIE tasks, which are the de-\\npendency about the order of text blocks and learning from a\\nfew training examples.\\nOver our experiments, we report the scores of LayoutLM\\nand LayoutLMv2 using the models published by the au-', metadata={'source': 'data/BROS.pdf', 'page': 4}),\n",
       " Document(page_content='few training examples.\\nOver our experiments, we report the scores of LayoutLM\\nand LayoutLMv2 using the models published by the au-\\nthors4and denote them as LayoutLM∗and LayoutLMv2∗.\\nWe report the mean (and optionally the standard deviation)\\nof the results using the 5 different random seeds.\\nWith the Order Information of Text Blocks Table 3\\nsummarizes the results for the FUNSD EE task reported by\\nprevious approaches. When comparing models only using\\ntext and layout, BROS shows remarkable performance im-\\nprovements by 2.51 (80.54 →83.05) for the BASE models\\nand 5.57 (78.95→84.52) for the LARGE models from the\\nprevious best. Interestingly, BROS provides better or similar\\nperformances compared to the multi-modal models incor-\\nporating additional visual ( Image∗) or hierarchical ( Cell∗)\\ninformation. In other words, although BROS does not re-\\nquire extra computations and parameters to process addi-\\ntional features, BROS can achieves better or comparable\\nperformances.', metadata={'source': 'data/BROS.pdf', 'page': 4}),\n",
       " Document(page_content='quire extra computations and parameters to process addi-\\ntional features, BROS can achieves better or comparable\\nperformances.\\nTable 4 shows the F1 scores on three EE and EL tasks with\\nthe order of text blocks given in the dataset. F, S, C, and Sci\\nrefer to FUNSD, SROIE∗, CORD, and SciTSR, respectively.\\nFor EE tasks, all models utilize BIO tagger that captures\\nspans of text blocks to represent key entities in documents.\\nFor EL tasks, SPADE decoder is used to identify relation-\\nships between entities not placed sequentially in a series of\\n3https://clova.ai/ocr\\n4https://github.com/microsoft/unilm', metadata={'source': 'data/BROS.pdf', 'page': 4}),\n",
       " Document(page_content='FUNSD EE\\nModel Modality Precision Recall F1 # Params\\nBERT BASE (Xu et al. 2020) Text 54.69 67.10 60.26 110M\\nLayoutLM BASE (Xu et al. 2020) Text + Layout 75.97 81.55 78.66 113M\\nDocFormer BASE (Appalaraju et al. 2021) Text + Layout 77.63 83.69 80.54 149M\\nBROS BASE (Ours) Text + Layout 81.16±0.33 85.02±0.32 83.05±0.26 110M\\nLayoutLM BASE (Xu et al. 2020) Text + Layout + Image * 76.77 81.95 79.27 160M\\nLayoutLMv2 BASE (Xu et al. 2021) Text + Layout + Image * 80.29 85.39 82.76 200M\\nDocFormer BASE (Appalaraju et al. 2021) Text + Layout + Image * 80.76 86.09 83.34 183M\\nSelfDoc (Li et al. 2021b) Text + Layout + Image * - - 83.36 137M\\nStrucTexT (Li et al. 2021c) Text + Layout + Image * 85.68 80.97 83.09 107M†\\nBERT LARGE (Xu et al. 2020) Text 61.13 70.85 65.63 340M\\nLayoutLM LARGE (Xu et al. 2021) Text + Layout 75.96 82.19 78.95 343M\\nBROS LARGE (Ours) Text + Layout 82.81±0.35 86.31±0.28 84.52±0.30 340M\\nLayoutLMv2 LARGE (Xu et al. 2021) Text + Layout + Image * 83.24 85.19 84.20 426M', metadata={'source': 'data/BROS.pdf', 'page': 5}),\n",
       " Document(page_content='BROS LARGE (Ours) Text + Layout 82.81±0.35 86.31±0.28 84.52±0.30 340M\\nLayoutLMv2 LARGE (Xu et al. 2021) Text + Layout + Image * 83.24 85.19 84.20 426M\\nStructuralLM LARGE (Li et al. 2021a) Text + Layout + Cell* 83.52 86.81 85.14 355M\\n†The number of parameters except for ResNet-FPN processing document images.\\nTable 3: Performance comparison on the FUNSD EE task. Bold indicates the best performance among models using only text\\nand layout, and underline represents the best one. Image * and Cell* denote additional visual and hierarchical information,\\nrespectively. Our methods are repeatedly evaluated ﬁve times and the values of other methods are the reported scores.\\nEntity Extraction Entity Linking\\nModel F S C F C Sci\\nBERT BASE 60.92 93.67 93.13 27.65 92.83 86.76\\nLayoutLM∗\\nBASE 78.54 95.11 96.26 45.86 95.21 99.05\\nLayoutLMv2∗\\nBASE 81.89 96.09 96.05 42.91 95.59 98.19\\nBROS BASE 83.05 96.28 96.50 71.46 95.73 99.45\\nBERT LARGE 64.17 94.25 94.74 29.11 94.31 89.23\\nLayoutLM∗', metadata={'source': 'data/BROS.pdf', 'page': 5}),\n",
       " Document(page_content='BASE 81.89 96.09 96.05 42.91 95.59 98.19\\nBROS BASE 83.05 96.28 96.50 71.46 95.73 99.45\\nBERT LARGE 64.17 94.25 94.74 29.11 94.31 89.23\\nLayoutLM∗\\nLARGE 79.27 95.36 96.12 42.83 95.41 99.33\\nLayoutLMv2∗\\nLARGE 83.59 96.39 97.24 70.57 97.29 99.76\\nBROS LARGE 84.52 96.62 97.28 77.01 97.40 99.58\\nTable 4: Performance comparisons on three EE and EL tasks\\nwith the order information of text blocks.\\ntext blocks. In all cases, BERT performs the worst because\\nthose tasks require understanding texts in 2D space, but\\nBERT only encodes 1D sequential information. LayoutLM∗\\nand LayoutLMv2∗show better performance than BERT\\nsince they encode layout features as well as text features.\\nAnd by combining visual features, LayoutLMv2∗performs\\nbetter than LayoutLM∗in most tasks. BROS shows the best\\nperformance in all tasks except SciTSR. It should be noted\\nthat the BROS BASE show better performance than that of\\nLayoutLM∗\\nLARGE , even though it uses three times lower num-', metadata={'source': 'data/BROS.pdf', 'page': 5}),\n",
       " Document(page_content='that the BROS BASE show better performance than that of\\nLayoutLM∗\\nLARGE , even though it uses three times lower num-\\nber of parameters (110M vs 343M). These results indicate\\nthat BROS effectively encodes the text and layout features.\\nWithout the Order Information of Text Blocks As we\\nmentioned in previous section, we introduce the permuted\\nKIE benchmarks lost the orders of text blocks by shufﬂing\\nthe provided orders. To solve EE and EL tasks without the\\norder information, we employ the SPADE decoder for all\\ntasks. Table 5 shows the comparison results. p-F, p-S, p-\\nC, and p-Sci refer to p-FUNSD, p-SROIE∗, p-CORD, andEntity Extraction Entity Linking\\nModel p-F p-S p-C p-F p-C p-Sci\\nBERT BASE 18.85 39.73 59.71 9.59 27.88 1.75\\nLayoutLM∗\\nBASE 33.89 66.05 80.86 22.98 61.51 97.32\\nLayoutLMv2∗\\nBASE 40.77 73.56 80.37 23.25 50.55 95.86\\nBROS BASE 76.94 82.85 95.86 69.61 87.72 99.19\\nBERT LARGE 18.10 43.19 57.17 10.81 27.12 1.93\\nLayoutLM∗\\nLARGE 33.11 56.84 82.88 20.72 61.98 97.64\\nLayoutLMv2∗', metadata={'source': 'data/BROS.pdf', 'page': 5}),\n",
       " Document(page_content='BERT LARGE 18.10 43.19 57.17 10.81 27.12 1.93\\nLayoutLM∗\\nLARGE 33.11 56.84 82.88 20.72 61.98 97.64\\nLayoutLMv2∗\\nLARGE 62.53 84.92 94.43 50.14 85.80 99.45\\nBROS LARGE 79.42 85.14 96.81 75.61 90.49 99.33\\nTable 5: Performance comparisons on three EE and EL tasks\\nwithout the order information of text blocks.\\nModel p- xy- yx- original\\nLayoutLM∗\\nBASE 33.89 34.02 55.47 78.47\\nLayoutLMv2∗\\nBASE 40.77 52.08 62.37 78.16\\nBROS BASE 76.94 77.16 77.42 81.61\\nLayoutLM∗\\nLARGE 33.11 33.54 41.45 48.30\\nLayoutLMv2∗\\nLARGE 62.53 69.14 75.45 83.00\\nBROS LARGE 79.42 79.91 80.02 83.23\\nTable 6: Comparison of FUNSD EE performances according\\nto sorting methods.\\np-SciTSR, respectively. BERT, which does not employ any\\nspatial information of text blocks, shows the worst results\\non the orderless conditions. By being aware of the spatial-\\nity, layout-aware language models show better performances\\nthan BERT, and BROS achieves the best except p-SciTSR.\\nMore interestingly, BROS shows minor performance drops', metadata={'source': 'data/BROS.pdf', 'page': 5}),\n",
       " Document(page_content='than BERT, and BROS achieves the best except p-SciTSR.\\nMore interestingly, BROS shows minor performance drops\\ncompared to Table 4, while LayoutLM∗and LayoutLMv2∗\\nsuffer from huge performance degradations by losing the or-', metadata={'source': 'data/BROS.pdf', 'page': 5}),\n",
       " Document(page_content='Figure 6: Performance comparisons according to the amount\\nof ﬁne-tuning data. Each point represents the result of ﬁne-\\ntuning using from 10% to 100% of training data.\\nDataset # Data BERT LayoutLM∗LayoutLMv2∗BROS\\nFUNSD 531.51 48.23 64.26 68.35\\nEE 1040.46 62.50 69.92 72.60\\nFUNSD 514.65 21.38 7.32 31.11\\nEL 1014.88 21.48 13.99 39.17\\nTable 7: Results of training with 5 and 10 examples.\\nder information of text blocks.\\nTo systematically investigate how the order information\\naffects the performance of the models, we construct vari-\\nants of FUNSD by re-ordering text blocks with two sort-\\ning methods based on the top-left points. The text blocks\\nof xy-FUNSD are sorted according to the x-axis with as-\\ncending order of y-axis and those of yx-FUNSD are sorted\\naccording to y-axis with ascending order of x-axis. Ta-\\nble 6 shows performance on p-FUNSD, xy-FUNSD, yx-\\nFUNSD, and the original FUNSD with the SPADE decoder.\\nIn our experiment, LayoutLM∗\\nLARGE achieves unstable per-', metadata={'source': 'data/BROS.pdf', 'page': 6}),\n",
       " Document(page_content='FUNSD, and the original FUNSD with the SPADE decoder.\\nIn our experiment, LayoutLM∗\\nLARGE achieves unstable per-\\nformance (48.30±12.51), when combined with SPADE de-\\ncoder. Interestingly, the performances of LayoutLM∗and\\nLayoutLMv2∗are degraded in the order of FUNSD, yx-\\nFUNSD, xy-FUNSD, and p-FUNSD as like the order of\\nthe reasonable serialization for text on 2D space. On the\\nother hand, the performance of BROS is relatively consis-\\ntent. These results show the robustness of BROS on multiple\\ntypes of serializers.\\nLearning from Few Training Examples One of the ad-\\nvantages of pre-trained models is that it shows effective\\ntransfer learning performance even with a few training ex-\\namples (Devlin et al. 2019). Since collecting ﬁne-tuning data\\nrequires a lot of resource, achieving high performance with\\na small number of training examples is important.\\nFigure 6 shows the results of the FUNSD KIE tasks by\\nvarying the amount of training examples from 10% to 100%', metadata={'source': 'data/BROS.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 6 shows the results of the FUNSD KIE tasks by\\nvarying the amount of training examples from 10% to 100%\\nduring ﬁne-tuning. In all models, performances tend to in-\\ncrease as the ratio of training data increased. In both tasks,\\nBROS shows the best performances regardless of the num-\\nber of training samples.\\nTo further test extreme cases, we conduct experiments us-\\ning only 5 and 10 training examples. Table 7 shows the re-\\nsults of the FUNSD KIE tasks. We ﬁne-tune models for 100\\nepochs with a batch size of 4. In all cases, BROS shows the\\nbest performances. The results prove the generalization abil-\\nity of BROS even with very few training examples.Entity Extraction Entity Linking\\nModel F S C F C Sci\\nLayoutLM†\\nBASE 76.89 94.99 94.37 44.00 93.60 99.06\\n→pos enc. only 78.84 95.45 96.36 59.92 94.83 99.22\\n→objectives only 78.44 94.81 95.95 47.22 94.11 99.20\\n→both (=BROS BASE)80.58 95.72 96.64 65.24 96.03 99.28\\nTable 8: Performance improvements on EE and EL tasks', metadata={'source': 'data/BROS.pdf', 'page': 6}),\n",
       " Document(page_content='→both (=BROS BASE)80.58 95.72 96.64 65.24 96.03 99.28\\nTable 8: Performance improvements on EE and EL tasks\\nthrough adding components of BROS. At the last line, all\\ncomponents are changed from LayoutLM and the model be-\\ncomes BROS.\\nEntity Extraction Entity Linking\\nSE F S C F C Sci\\nAbsolute 78.44 94.81 95.95 47.22 94.11 99.20\\nLayoutLMv2’s 78.93 94.71 95.82 53.57 95.27 99.28\\nOurs 80.58 95.72 96.64 65.24 96.03 99.28\\nTable 9: Spatial encoding methods from BROS’ setting.\\nAblation Study\\nWe conduct ablation studies to investigate which component\\ncontributes the performance improvement. For the ablation\\nstudies, we utilize LayoutLM†that is our own implemen-\\ntation of LayoutLM for fair comparisons under the same\\nexperimental settings. All models in these studies are pre-\\ntrained for 1 epoch.\\nTable 8 provides performance changes from adding our\\nproposed components. When applying our proposed posi-\\ntional encoding to LayoutLM, the performances consistently', metadata={'source': 'data/BROS.pdf', 'page': 6}),\n",
       " Document(page_content='proposed components. When applying our proposed posi-\\ntional encoding to LayoutLM, the performances consistently\\nincrease with huge margins of 3.62pp on average over all\\ntasks. Independently, our extension on pre-training objec-\\ntives solely provides 1.14pp of performance improvement\\non average. By utilizing both, BROS BASE provides the best\\nperformances with margins of 5.10pp on average. This abla-\\ntion study proves that each component of BROS solely con-\\ntributes to performance improvements as well as their com-\\nbination provides better results.\\nTable 9 compares three positional encoding methods:\\nabsolute position in LayoutLM, relative position in Lay-\\noutLMv2, and ours. Relative position methods perform bet-\\nter than absolute one and the performance gap becomes\\nlarger in EL tasks. And among them, our method shows the\\nbest results.\\nConclusion\\nWe propose a pre-trained language model, BROS, which\\nfocuses on modeling text and layout features for effective', metadata={'source': 'data/BROS.pdf', 'page': 6}),\n",
       " Document(page_content='best results.\\nConclusion\\nWe propose a pre-trained language model, BROS, which\\nfocuses on modeling text and layout features for effective\\nkey information extraction from documents. By encoding\\ntexts in 2D space with their relative positions and pre-\\ntraining the model with the area-masking strategy, BROS\\nshows superior performance without relying on any addi-\\ntional visual features. In addition, under the two real-world\\nsettings–imprecise text serialization and small amount of\\ntraining examples–BROS shows robust performance while\\nother models show signiﬁcant performance degradation.', metadata={'source': 'data/BROS.pdf', 'page': 6}),\n",
       " Document(page_content='Acknowledgments\\nWe thank many colleagues at NA VER CLOV A for their help,\\nin particular Yoonsik Kim, Moonbin Yim, Han-cheol Cho,\\nBado Lee, Seunghyun Park, and Youngmin Baek for useful\\ndiscussions.\\nReferences\\nAppalaraju, S.; Jasani, B.; Kota, B. U.; Xie, Y .; and\\nManmatha, R. 2021. DocFormer: End-to-End Trans-\\nformer for Document Understanding. arXiv preprint\\narXiv:2106.11539 .\\nChi, Z.; Huang, H.; Xu, H.-D.; Yu, H.; Yin, W.; and Mao,\\nX.-L. 2019. Complicated table structure recognition. arXiv\\npreprint arXiv:1908.04729 .\\nClausner, C.; Pletschacher, S.; and Antonacopoulos, A.\\n2013. The signiﬁcance of reading order in document recog-\\nnition and its evaluation. In 2013 12th International Con-\\nference on Document Analysis and Recognition (ICDAR) ,\\n688–692. IEEE.\\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q.; and\\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\\nguage Models beyond a Fixed-Length Context. In Proceed-\\nings of the 57th Annual Meeting of the Association for Com-', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='guage Models beyond a Fixed-Length Context. In Proceed-\\nings of the 57th Annual Meeting of the Association for Com-\\nputational Linguistics (ACL) .\\nDenk, T. I.; and Reisswig, C. 2019. BERTgrid: Contex-\\ntualized Embedding for 2D Document Representation and\\nUnderstanding. In Workshop on Document Intelligence at\\nNeurIPS 2019 .\\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technolo-\\ngies (NAACL-HLT), Volume 1 (Long and Short Papers) ,\\n4171–4186.\\nHarley, A. W.; Ufkes, A.; and Derpanis, K. G. 2015. Eval-\\nuation of deep convolutional nets for document image clas-\\nsiﬁcation and retrieval. In Proceedings of the 13th Interna-\\ntional Conference on Document Analysis and Recognition\\n(ICDAR) , 991–995.\\nHuang, Z.; Chen, K.; He, J.; Bai, X.; Karatzas, D.; Lu, S.;', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='tional Conference on Document Analysis and Recognition\\n(ICDAR) , 991–995.\\nHuang, Z.; Chen, K.; He, J.; Bai, X.; Karatzas, D.; Lu, S.;\\nand Jawahar, C. 2019. ICDAR2019 competition on scanned\\nreceipt ocr and information extraction. In Proceedings of the\\n15th International Conference on Document Analysis and\\nRecognition (ICDAR) , 1516–1520. IEEE.\\nHwang, W.; Kim, S.; Seo, M.; Yim, J.; Park, S.; Park, S.;\\nLee, J.; Lee, B.; and Lee, H. 2019. Post-OCR parsing: build-\\ning simple and robust parser via BIO tagging. In Workshop\\non Document Intelligence at NeurIPS 2019 .\\nHwang, W.; Yim, J.; Park, S.; Yang, S.; and Seo, M. 2021.\\nSpatial Dependency Parsing for Semi-Structured Document\\nInformation Extraction. In Findings of the Association for\\nComputational Linguistics: ACL-IJCNLP 2021 , 330–343.\\nJaume, G.; Ekenel, H. K.; and Thiran, J.-P. 2019. FUNSD:\\nA dataset for form understanding in noisy scanned doc-', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='Jaume, G.; Ekenel, H. K.; and Thiran, J.-P. 2019. FUNSD:\\nA dataset for form understanding in noisy scanned doc-\\numents. In 2019 International Conference on DocumentAnalysis and Recognition Workshops (ICDARW) , volume 2,\\n1–6. IEEE.\\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\\nand Levy, O. 2020. SpanBERT: Improving pre-training by\\nrepresenting and predicting spans. Transactions of the Asso-\\nciation for Computational Linguistics (TACL) , 8: 64–77.\\nLewis, D.; Agam, G.; Argamon, S.; Frieder, O.; Grossman,\\nD.; and Heard, J. 2006. Building a test collection for com-\\nplex document information processing. In Proceedings of\\nthe 29th annual international ACM SIGIR conference on\\nResearch and development in information retrieval (SIGIR) ,\\n665–666.\\nLi, C.; Bi, B.; Yan, M.; Wang, W.; Huang, S.; Huang, F.; and\\nSi, L. 2021a. StructuralLM: Structural Pre-training for Form\\nUnderstanding. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguistics and the', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='Understanding. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguistics and the\\n11th International Joint Conference on Natural Language\\nProcessing (ACL-IJCNLP) , 6309–6318.\\nLi, L.; Gao, F.; Bu, J.; Wang, Y .; Yu, Z.; and Zheng, Q. 2020.\\nAn End-to-End OCR Text Re-organization Sequence Learn-\\ning for Rich-text Detail Image Comprehension. In Proceed-\\nings of the 16th European Conference on Computer Vision\\n(ECCV) .\\nLi, P.; Gu, J.; Kuen, J.; Morariu, V . I.; Zhao, H.; Jain,\\nR.; Manjunatha, V .; and Liu, H. 2021b. SelfDoc: Self-\\nSupervised Document Representation Learning. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , 5652–5660.\\nLi, Y .; Qian, Y .; Yu, Y .; Qin, X.; Zhang, C.; Liu, Y .; Yao, K.;\\nHan, J.; Liu, J.; and Ding, E. 2021c. StrucTexT: Structured\\nText Understanding with Multi-Modal Transformers. arXiv\\npreprint arXiv:2108.02923 .\\nLiu, X.; Gao, F.; Zhang, Q.; and Zhao, H. 2019. Graph Con-', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='Text Understanding with Multi-Modal Transformers. arXiv\\npreprint arXiv:2108.02923 .\\nLiu, X.; Gao, F.; Zhang, Q.; and Zhao, H. 2019. Graph Con-\\nvolution for Multimodal Information Extraction from Visu-\\nally Rich Documents. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies\\n(NAACL-HLT), Volume 2 (Industry Papers) , 32–39.\\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\\ncay Regularization. In Proceedings of the 7th International\\nConference on Learning Representations (ICLR) .\\nPark, S.; Shin, S.; Lee, B.; Lee, J.; Surh, J.; Seo, M.; and\\nLee, H. 2019. CORD: A Consolidated Receipt Dataset for\\nPost-OCR Parsing. In Workshop on Document Intelligence\\nat NeurIPS 2019 .\\nPowalski, R.; Borchmann, Ł.; Jurkiewicz, D.; Dwojak, T.;\\nPietruszka, M.; and Pałka, G. 2021. Going Full-TILT Boo-\\ngie on Document Understanding with Text-Image-Layout\\nTransformer. arXiv preprint arXiv:2102.09550 .', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='gie on Document Understanding with Text-Image-Layout\\nTransformer. arXiv preprint arXiv:2102.09550 .\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\\ntention is all you need. In Advances in Neural Information\\nProcessing Systems 30 (NeurIPS) , 5998–6008.\\nWang, Z.; Xu, Y .; Cui, L.; Shang, J.; and Wei, F. 2021. Lay-\\noutReader: Pre-training of Text and Layout for Reading Or-\\nder Detection. arXiv:2108.11591.', metadata={'source': 'data/BROS.pdf', 'page': 7}),\n",
       " Document(page_content='Xu, Y .; Li, M.; Cui, L.; Huang, S.; Wei, F.; and Zhou, M.\\n2020. LayoutLM: Pre-training of text and layout for docu-\\nment image understanding. In Proceedings of the 26th ACM\\nSIGKDD International Conference on Knowledge Discov-\\nery & Data Mining (KDD) , 1192–1200.\\nXu, Y .; Xu, Y .; Lv, T.; Cui, L.; Wei, F.; Wang, G.; Lu, Y .; Flo-\\nrencio, D.; Zhang, C.; Che, W.; et al. 2021. LayoutLMv2:\\nMulti-modal Pre-training for Visually-Rich Document Un-\\nderstanding. In Proceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language Pro-\\ncessing (ACL-IJCNLP) , 2579–2591.', metadata={'source': 'data/BROS.pdf', 'page': 8}),\n",
       " Document(page_content='Two Categories of Document KIE tasks and Parsers for Them\\nDocument KIE tasks can be categorized into two downstream tasks: (1) an entity extraction (EE) task and (2) an entity linking\\n(EL) task. The EE task identiﬁes sequences of text blocks that represent desired target texts (e.g. extracts menu name in a\\nreceipt) and the EL task connects key entities through their hierarchical or semantic relations (e.g. connects menu name and its\\nprice).\\nTo address the EE and EL tasks, we introduce two parsers. One is a sequence classiﬁer, BIO tagger, that can operate based\\non prior information about an order of text blocks. It has been utilized as a conventional method for extracting entities from\\nthe text sequences. The other is a graph-based parser, SPADE (Hwang et al. 2021) decoder, that does not require any order\\ninformation of text blocks. Since it directly identifying relations between tokens, it can extract entities whether the order of', metadata={'source': 'data/BROS.pdf', 'page': 9}),\n",
       " Document(page_content='information of text blocks. Since it directly identifying relations between tokens, it can extract entities whether the order of\\ntext blocks is provided or not. It can also perform the task which requires inferring the relationship between text blocks. The\\nfollowing sections brieﬂy introduce the two types of parsers.\\nBIO Tagger\\nBIO tagger, a representative parser that depends on the proper order of text blocks, extracts key information by identifying\\nthe beginning (B) and inside (I) points of the ordered text blocks. The proper order of text blocks indicates an order in which\\nall of the key information can be represented in its sub-sequences. The sequence classiﬁer requires this condition because it\\nnever succeeds to ﬁnd the key information with a wrong sequence. For example, if three text blocks, “optical”, “character”, and\\n“recognition”, are ordered as “recognition”, “optical”, and “character”, the sequence classiﬁer cannot ﬁnd “optical character\\nrecognition”.', metadata={'source': 'data/BROS.pdf', 'page': 9}),\n",
       " Document(page_content='“recognition”, are ordered as “recognition”, “optical”, and “character”, the sequence classiﬁer cannot ﬁnd “optical character\\nrecognition”.\\n(a) Recognized text blocks.\\n (b) Serialized text blocks.\\n (c) BIO-tagged text sequence.\\nFigure 7: Visual descriptions of how BIO tagger extracts entities in a document. All recognized tokens are serialized and\\nclassiﬁed. By combining sub-sequences identiﬁed by the BIO taggings, key information can be parsed from the recognized\\ntokens.\\nFigure 7 shows how the BIO tagger performs the EE task for a given document. First, text blocks are recognized by an OCR\\nengine (Figure 7, a). The recognized text blocks are then serialized by a serializer (Figure 7, b). Finally, for each token, the BIO\\nclasses are classiﬁed and key information is extracted by combining the classiﬁed labels (Figure 7, c).\\nThe BIO tagger cannot solve the EL task since links between text blocks cannot be represented as a sequence unit. In', metadata={'source': 'data/BROS.pdf', 'page': 9}),\n",
       " Document(page_content='The BIO tagger cannot solve the EL task since links between text blocks cannot be represented as a sequence unit. In\\naddition, a single text block can hold the same relationships with other multiple text blocks but the sequence-based approach\\ncannot explain the one-to-many relations as well.\\nSPADE Decoder\\nIn many practical cases, the proper order of text blocks cannot be available. Most OCR APIs provide the order of text blocks\\nbased on rule-based approaches but they cannot guarantee the proper order of text blocks (Clausner, Pletschacher, and Antona-\\ncopoulos 2013; Li et al. 2020; Wang et al. 2021).\\nHere, we utilize SPADE (Hwang et al. 2021) decoder to extract key information without any information about the order.\\nThe key idea of SPADE decoder is to extract a directional sub-graph from a fully-connected graph which nodes are text blocks.\\nDue to no limitation on the connections between text blocks, it does not require order information. In our paper, we slightly', metadata={'source': 'data/BROS.pdf', 'page': 9}),\n",
       " Document(page_content='Due to no limitation on the connections between text blocks, it does not require order information. In our paper, we slightly\\nmodiﬁed the SPADE decoder for its application to the pre-trained models. The details are described as the followings.\\nFor EE tasks, the SPADE decoder divides the problem into two sub-tasks: initial token classiﬁcation (Figure 8, a) and\\nsubsequent token classiﬁcation (Figure 8, b). Let ˜ti∈RHdenote theithtoken representation from the last Transformer layer of\\nthe pre-trained model. The initial token classiﬁcation conducts a token-level tagging to determine whether a token is an initial\\ntoken of target information as follows,\\npitc(˜ti) =softmax (Witc˜ti), (3)\\nwhere Witc∈R(C+1)×His a linear transition matrix and Cindicates the number of target classes. Here, the extra +1 dimension\\nis considered to indicate non-initial tokens.', metadata={'source': 'data/BROS.pdf', 'page': 9}),\n",
       " Document(page_content='(a) Initial token classiﬁcation\\n (b) Subsequent token classiﬁcation\\n (c) Entity linking (EL) task\\nFigure 8: Visual descriptions of SPADE decoder downstream tasks. For EE tasks, SPADE decoder combines two sub-tasks\\nsuch as (a) and (b). SPADE decoder identiﬁes initial tokens and then connects next tokens without any order information of text\\nblocks. For the EL task, SPADE decoder links the ﬁrst tokens of the entities.\\nThe subsequent token classiﬁcation is conducted by utilizing pair-wise token representations as follows,\\npstc(˜ti) =softmax ((Wstc-s˜ti)⊤Tstc)⊤,\\nwhere Tstc= [tstc;Wstc-t˜t1;...;Wstc-t˜tN].\\nHere, Wstc-s,Wstc-t∈RHstc×Hare linear transition matrices, Hstcis a hidden feature dimension for the subsequent token\\nclassiﬁcation decoder and Nis the maximum number of tokens. The semicolon (;) indicates concatenation. tstc∈RHstcis a\\nmodel parameter to classify tokens which do not have a subsequent token or are not related to any class. It has a similar role', metadata={'source': 'data/BROS.pdf', 'page': 10}),\n",
       " Document(page_content='model parameter to classify tokens which do not have a subsequent token or are not related to any class. It has a similar role\\nwith an end-of-sequence token, [EOS] , in NLP. By solving these two sub-tasks, the SPADE decoder can identify a sequence\\nof text blocks by ﬁnding initial tokens and then connecting subsequent tokens.\\nFor EL tasks, the SPADE decoder conducts a binary classiﬁcation for all possible pairs of tokens (Figure 8, c) as follows,\\nprel(˜ti,˜tj) =sigmoid ((Wrel-s˜ti)⊤(Wrel-t˜tj)), (4)\\nwhere Wrel-s,Wrel-t∈RHrel×Hare linear transition matrices and Hrelis a hidden feature dimension. Compared to the sub-\\nsequent token classiﬁcation, a single token can hold multiple relations with other tokens to represent hierarchical structures of\\ndocument layouts.\\nIn our experiments, HstcandHrelare set to 128 for FUNSD, 64 for SROIE∗, and 256 for CORD and SciTSR.\\nKIE Benchmark Datasets\\nHere, we describe three EE tasks and three EL tasks from four KIE benchmark datasets.', metadata={'source': 'data/BROS.pdf', 'page': 10}),\n",
       " Document(page_content='KIE Benchmark Datasets\\nHere, we describe three EE tasks and three EL tasks from four KIE benchmark datasets.\\n• Form Understanding in Noisy Scanned Documents (FUNSD) (Jaume, Ekenel, and Thiran 2019) is a set of documents with\\nvarious forms. The dataset consists of 149 training and 50 testing examples. FUNSD has both EE and EL tasks. In the EE\\ntask, there are three semantic entities: Header, Question, and Answer. In the EL task, the semantic hierarchies are represented\\nas relations between text blocks like header-question and question-answer pairs.\\n• SROIE∗is a variant of Task 3 of “Scanned Receipts OCR and Information Extraction” (SROIE)5that consists of a set\\nof store receipts. In the original SROIE task, semantic contents (Company, Date, Address, and Total price) are generated\\nwithout explicit connection to the text blocks. To convert SROIE into a EE task, we developed SROIE∗by matching ground', metadata={'source': 'data/BROS.pdf', 'page': 10}),\n",
       " Document(page_content='without explicit connection to the text blocks. To convert SROIE into a EE task, we developed SROIE∗by matching ground\\ntruth contents with text blocks. We also split the original training set into 526 training and 100 testing examples because the\\nground truths are not given in the original test set. SROIE∗will be publicly available.\\n• Consolidated Receipt Dataset (CORD) (Park et al. 2019) is a set of store receipts with 800 training, 100 validation, and\\n100 testing examples. CORD consists of both EE and EL tasks. In the EE task, there are 30 semantic entities including\\nmenu name, menu price, and so on. In the EL task, the semantic entities are linked according to their layout structure. For\\nexample, menu name entities are linked to menu id, menu count, and menu price.\\n• Complicated Table Structure Recognition (SciTSR) (Chi et al. 2019) is an EL task that connects cells in a table to recognize', metadata={'source': 'data/BROS.pdf', 'page': 10}),\n",
       " Document(page_content='• Complicated Table Structure Recognition (SciTSR) (Chi et al. 2019) is an EL task that connects cells in a table to recognize\\nthe table structure. There are two types of relations: vertical and horizontal connections between cells. The dataset consists\\nof 12,000 training images and 3,000 test images.\\nFigure 9 shows the sample images of these benchmark datasets.\\n5https://rrc.cvc.uab.es/?ch=13', metadata={'source': 'data/BROS.pdf', 'page': 10}),\n",
       " Document(page_content='(a) FUNSD\\n(b) SROIE\\n(c) CORD\\n(d) SciTSR\\nFigure 9: The sample images of four KIE benchmark datasets.', metadata={'source': 'data/BROS.pdf', 'page': 11}),\n",
       " Document(page_content='# Pre-training data # Epochs Model Precision Recall F1\\n500K 1LayoutLMBASE (Xu et al. 2020) 0.5779 0.6955 0.6313\\nLayoutLM†\\nBASE 0.5823 0.6935 0.6330\\n1M 1LayoutLMBASE (Xu et al. 2020) 0.6156 0.7005 0.6552\\nLayoutLM†\\nBASE 0.6142 0.7151 0.6608\\n2M 1LayoutLMBASE (Xu et al. 2020) 0.6599 0.7355 0.6957\\nLayoutLM†\\nBASE 0.6562 0.7456 0.6980\\n11M1LayoutLMBASE (Xu et al. 2020) 0.7464 0.7815 0.7636\\nLayoutLM†\\nBASE 0.7384 0.8022 0.7689\\n2LayoutLMBASE (Xu et al. 2020) 0.7597 0.8155 0.7866\\nLayoutLM†\\nBASE 0.7612 0.8188 0.7889\\nTable 10: Sanity checking of LayoutLM†by comparing its performances on FUNSD EE task from the reported scores in Xu\\net al. (2020). LayoutLM†is trained in our experimental settings including pre-training datasets and hardware devices.\\nBIO tagger SPADE decoder\\nModel F S C F S C\\nBERT BASE 60.92 93.67 93.13 63.38 93.09 95.16\\nLayoutLM∗\\nBASE 78.54 95.11 96.26 78.47 93.33 96.71\\nLayoutLMv2∗\\nBASE 81.89 96.09 96.05 78.16 95.32 96.13\\nBROS BASE 83.05 96.28 96.50 81.61 95.70 96.73', metadata={'source': 'data/BROS.pdf', 'page': 12}),\n",
       " Document(page_content='LayoutLM∗\\nBASE 78.54 95.11 96.26 78.47 93.33 96.71\\nLayoutLMv2∗\\nBASE 81.89 96.09 96.05 78.16 95.32 96.13\\nBROS BASE 83.05 96.28 96.50 81.61 95.70 96.73\\nBERT LARGE 64.17 94.25 94.74 65.23 93.40 95.30\\nLayoutLM∗\\nLARGE 79.27 95.36 96.12 48.30 93.90 95.63\\nLayoutLMv2∗\\nLARGE 83.59 96.39 97.24 83.00 96.62 97.18\\nBROS LARGE 84.52 96.62 97.28 83.23 96.69 97.32\\nTable 11: Performance comparisons of BIO tagger and SPADE decoder for the three EE tasks with the order information of text\\nblocks.\\nModel Speed (BASE) Speed (LARGE)\\nLayoutLM∗0.0413s±0.0008 0.1182s±0.0027\\nLayoutLMv2∗0.1438s±0.0009 0.2330s±0.0011\\nBROS 0.0721s±0.0007 0.1794s±0.0021\\nTable 12: Inference speed (Avg. of 50 samples on T4 GPU).\\nCompare Published LayoutLM Model and Our Own Implementation\\nTable 10 compares our implementation of LayoutLM from the reported scores in Xu et al. (2020). As can be seen, multiple\\nexperiments are conducted according to the number of pre-training data. Our implementation, referred to LayoutLM†, shows', metadata={'source': 'data/BROS.pdf', 'page': 12}),\n",
       " Document(page_content='experiments are conducted according to the number of pre-training data. Our implementation, referred to LayoutLM†, shows\\ncomparable performances over all settings.\\nCompare BIO Tagger and SPADE Decoder\\nTable 11 shows the performance comparisons of BIO tagger and SPADE decoder for the three EE tasks when the order of text\\nblocks is given. There is not much difference in performance between the BIO tagger and the SPADE decoder in this case.\\nInterestingly, the performance of BERT on FUNSD and CORD is higher when using SPADE decoder than when using BIO\\ntagger. This seems to be because 2D layout information can be utilized by using the SPADE decoder. In FUNSD EE task, the\\nperformance of LayoutLM∗\\nLARGE is unstable to have a standard deviation of 12.51. BROS shows the best performance in all\\ntasks regardless of the parsers.\\nCompare the Inference Speed of the Models\\nTable 12 shows the inference speed of the models. Since BROS considers relative positions for all text block pairs, it is slower', metadata={'source': 'data/BROS.pdf', 'page': 12}),\n",
       " Document(page_content='Table 12 shows the inference speed of the models. Since BROS considers relative positions for all text block pairs, it is slower\\nthan LayoutLM, but faster than LayoutLMv2 using image features. It should be noted that BROS shows the best performance\\namong them.', metadata={'source': 'data/BROS.pdf', 'page': 12}),\n",
       " Document(page_content='Figure 10: Results of rotated FUNSD EE task.\\nExperiments on Rotated Images\\nTo test in another imprecise text serialization setup, we conducted experiment on the rotated images in which the image and\\nlocations of all text blocks are rotated and the block order is re-serialized (yx-). Figure 10 shows the results with the same trend\\nas Table 6.', metadata={'source': 'data/BROS.pdf', 'page': 13})]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token splitting\n",
    "\n",
    "We can also split on token count explicity, if we want.\n",
    "\n",
    "This can be useful because LLMs often have context windows designated in tokens.\n",
    "\n",
    "Tokens are often ~4 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"foo bar bazzyfoo\"\n",
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='BR', metadata={'source': 'data/BROS.pdf', 'page': 0})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(pages)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/BROS.pdf', 'page': 0}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context aware splitting\n",
    "\n",
    "Chunking aims to keep text with common context together.\n",
    "\n",
    "A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
    "\n",
    "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Hi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Hi this is Lance', metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
